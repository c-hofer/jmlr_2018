{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Found data!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from chofer_torchex.utils.data.collate import dict_sample_target_iter_concat\n",
    "from chofer_torchex.utils.functional import collection_cascade, cuda_cascade\n",
    "from jmlr_2018_code.datasets import Mpeg7\n",
    "from jmlr_2018_code.utils import *\n",
    "from chofer_torchex.nn.slayer import SLayerExponential, SLayerRational, LogStretchedBirthLifeTimeCoordinateTransform, prepare_batch, SLayerRationalHat\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from collections import Counter, defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(0)\n",
    "\n",
    "\n",
    "class train_env:\n",
    "    nu = 0.01\n",
    "    n_epochs = 600\n",
    "    lr_initial = 0.01\n",
    "    momentum = 0.9\n",
    "    lr_epoch_step = 100\n",
    "    batch_size = 100\n",
    "    train_size = 0.9\n",
    "    \n",
    "\n",
    "coordinate_transform = LogStretchedBirthLifeTimeCoordinateTransform(nu=train_env.nu)\n",
    "            \n",
    "\n",
    "used_directions = ['dim_0_dir_{}'.format(i) for i in range(0, 32,2)]\n",
    "mpeg7_data_set = Mpeg7(root_dir='./data')\n",
    "mpeg7_data_set.sample_transforms = [\n",
    "                                    lambda x: {k: x[k] for k in used_directions}, \n",
    "                                    numpy_to_torch_cascade,\n",
    "                                    lambda x: collection_cascade(x, \n",
    "                                                                 lambda x: isinstance(x, torch._TensorBase), \n",
    "                                                                 lambda x: coordinate_transform(x))\n",
    "                                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PHTCollate:   \n",
    "    def __init__(self, nu, cuda=True, rotation_augmentation=False):\n",
    "        self.cuda = cuda\n",
    "        self.rotation_augmentation = rotation_augmentation\n",
    "        \n",
    "    def __call__(self, sample_target_iter):\n",
    "        \n",
    "        augmented_samples = []\n",
    "        if self.rotation_augmentation:\n",
    "            samples, targets = [], []\n",
    "            for x, y in sample_target_iter:                \n",
    "                i = random.randint(0, len(used_directions)-1)\n",
    "                shifted_keys = used_directions[i:] + used_directions[:i]                \n",
    "                \n",
    "                samples.append({k: x[ki] for k, ki in zip(used_directions, shifted_keys)})\n",
    "                targets.append(y)\n",
    "                \n",
    "            sample_target_iter = zip(samples, targets)\n",
    "\n",
    "        x, y = dict_sample_target_iter_concat(sample_target_iter)                                            \n",
    "                                              \n",
    "        for k in x.keys():\n",
    "            batch_view = x[k]\n",
    "            x[k] = prepare_batch(batch_view, 2)                  \n",
    "\n",
    "        y = torch.LongTensor(y)    \n",
    "\n",
    "        if self.cuda:\n",
    "            # Shifting the necessary parts of the prepared batch to the cuda\n",
    "            x = {k: collection_cascade(v,\n",
    "                                       lambda x: isinstance(x, tuple),\n",
    "                                       lambda x: (x[0].cuda(), x[1].cuda(), x[2], x[3]))\n",
    "                 for k, v in x.items()}\n",
    "\n",
    "            y = y.cuda()\n",
    "\n",
    "        return x, y                       \n",
    "    \n",
    "collate_fn_train = PHTCollate(train_env.nu, cuda=True)\n",
    "collate_fn_test = PHTCollate(train_env.nu, cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated training and testing split:\n",
      "Train: Counter({44: 18, 57: 18, 58: 18, 60: 18, 47: 18, 19: 18, 25: 18, 21: 18, 64: 18, 68: 18, 43: 18, 8: 18, 15: 18, 23: 18, 38: 18, 16: 18, 12: 18, 31: 18, 32: 18, 9: 18, 24: 18, 14: 18, 22: 18, 18: 18, 6: 18, 66: 18, 0: 18, 50: 18, 1: 18, 48: 18, 27: 18, 39: 18, 45: 18, 53: 18, 11: 18, 59: 18, 56: 18, 28: 18, 42: 18, 35: 18, 51: 18, 26: 18, 46: 18, 67: 18, 41: 18, 54: 18, 4: 18, 49: 18, 34: 18, 5: 18, 13: 18, 63: 18, 40: 18, 65: 18, 61: 18, 69: 18, 10: 18, 62: 18, 52: 18, 33: 18, 7: 18, 2: 18, 30: 18, 36: 18, 3: 18, 37: 18, 17: 18, 29: 18, 55: 18, 20: 18})\n",
      "Test: Counter({59: 2, 47: 2, 67: 2, 29: 2, 1: 2, 56: 2, 19: 2, 34: 2, 36: 2, 6: 2, 46: 2, 60: 2, 25: 2, 53: 2, 43: 2, 37: 2, 18: 2, 31: 2, 14: 2, 64: 2, 39: 2, 68: 2, 26: 2, 7: 2, 63: 2, 10: 2, 48: 2, 69: 2, 9: 2, 66: 2, 50: 2, 2: 2, 52: 2, 55: 2, 17: 2, 42: 2, 33: 2, 5: 2, 22: 2, 40: 2, 4: 2, 35: 2, 61: 2, 0: 2, 16: 2, 51: 2, 32: 2, 21: 2, 23: 2, 15: 2, 38: 2, 49: 2, 57: 2, 44: 2, 45: 2, 28: 2, 3: 2, 24: 2, 8: 2, 62: 2, 58: 2, 12: 2, 54: 2, 11: 2, 41: 2, 20: 2, 65: 2, 30: 2, 27: 2, 13: 2})\n"
     ]
    }
   ],
   "source": [
    "train_sampler, test_sampler = get_train_test_sampler(mpeg7_data_set, train_env.train_size, stratified=True)\n",
    "\n",
    "dl_train = DataLoader(mpeg7_data_set,\n",
    "                      batch_size=train_env.batch_size, \n",
    "                      collate_fn=collate_fn_train,\n",
    "                      sampler=train_sampler)\n",
    "\n",
    "dl_test = DataLoader(mpeg7_data_set,\n",
    "                     batch_size=train_env.batch_size, \n",
    "                     collate_fn=collate_fn_test, \n",
    "                     sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Slayer(n_elements):\n",
    "    return SLayerRationalHat(n_elements, radius_init=0.1)\n",
    "#     return SLayerRational(n_elements=n_elements, \n",
    "#                           point_dimension=2, \n",
    "#                           sharpness_init=25, \n",
    "#                           exponent_init=1, \n",
    "#                           share_sharpness=False,\n",
    "#                           share_exponent=False,\n",
    "#                           pointwise_activation_threshold=None,\n",
    "#                           freeze_exponent=False\n",
    "#                           )\n",
    "\n",
    "#     return SLayerExponential(n_elements=n_elements, point_dimension=2)\n",
    "\n",
    "\n",
    "def LinearCell(n_in, n_out):\n",
    "    m = nn.Sequential(nn.Linear(n_in, n_out), \n",
    "                      nn.BatchNorm1d(n_out), \n",
    "                      nn.ELU()\n",
    "                     )\n",
    "    m.out_features = m[0].out_features\n",
    "    return m\n",
    "\n",
    "\n",
    "class Mpeg7_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()   \n",
    "        self.n_elements = 150\n",
    "        \n",
    "        self.reccurent_in_dim = self.n_elements #int(self.n_elements/2)\n",
    "        self.reccurent_out_dim = self.n_elements #int(self.n_elements/2)\n",
    "        self.batch_size = train_env.batch_size\n",
    "        \n",
    "        self.slayers = ModuleDict()\n",
    "#         self.slayers_2_recurrent_linears = ModuleDict()\n",
    "        for k in used_directions:\n",
    "            s = Slayer(self.n_elements)\n",
    "            self.slayers[k] = s            \n",
    "#             l = nn.Linear(s.n_elements, self.reccurent_in_dim)\n",
    "#             self.slayers_2_recurrent_linears[k] = LinearCell(self.n_elements, self.reccurent_in_dim)\n",
    "            \n",
    "        self.recurrent = nn.LSTM(self.reccurent_in_dim, self.reccurent_out_dim)\n",
    "        \n",
    "        n_1 =1000\n",
    "        self.cls = nn.Sequential(\n",
    "                                 nn.Dropout(0.4),\n",
    "                                 nn.BatchNorm1d(self.reccurent_out_dim),\n",
    "                                 LinearCell(self.reccurent_out_dim, n_1),\n",
    "                                 nn.Dropout(0.3),\n",
    "                                 LinearCell(n_1, int(n_1/2)),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(int(n_1/2), 70)\n",
    "                                )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = []\n",
    "        for k in used_directions:            \n",
    "            xx = self.slayers[k](input[k])\n",
    "#             xx = self.slayers_2_recurrent_linears[k](xx)\n",
    "            x.append(xx)\n",
    "\n",
    "        x = torch.stack(x, dim=0)  \n",
    "        x = torch.cat([x]*10, dim=0)\n",
    "        \n",
    "        x, _ = self.recurrent(x)\n",
    "        \n",
    "        x = x[-1]\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        x = self.cls(x)        \n",
    "               \n",
    "        return x\n",
    "    \n",
    "    def parameters_split(self):\n",
    "        return {'non_linear': self.slayers.parameters(),\n",
    "                'linear': itertools.chain(self.cls.parameters(), self.recurrent.parameters())}\n",
    "    \n",
    "    def center_init(self, sample_target_iter):\n",
    "        centers = k_means_center_init(sample_target_iter, self.n_elements)\n",
    "        \n",
    "        for k, v in centers.items():\n",
    "            self.slayers._modules[k].centers.data = v\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600, Batch 13/13       \n",
      " testing...\n",
      "0.02142857142857143\n",
      "Epoch 2/600, Batch 13/13       \n",
      " testing...\n",
      "0.07142857142857142\n",
      "Epoch 3/600, Batch 13/13       \n",
      " testing...\n",
      "0.07857142857142857\n",
      "Epoch 4/600, Batch 13/13       \n",
      " testing...\n",
      "0.14285714285714285\n",
      "Epoch 5/600, Batch 13/13       \n",
      " testing...\n",
      "0.24285714285714285\n",
      "Epoch 6/600, Batch 13/13       \n",
      " testing...\n",
      "0.2857142857142857\n",
      "Epoch 7/600, Batch 13/13       \n",
      " testing...\n",
      "0.37142857142857144\n",
      "Epoch 8/600, Batch 13/13       \n",
      " testing...\n",
      "0.45\n",
      "Epoch 9/600, Batch 13/13       \n",
      " testing...\n",
      "0.39285714285714285\n",
      "Epoch 10/600, Batch 13/13       \n",
      " testing...\n",
      "0.35\n",
      "Epoch 11/600, Batch 13/13       \n",
      " testing...\n",
      "0.5071428571428571\n",
      "Epoch 12/600, Batch 13/13       \n",
      " testing...\n",
      "0.4928571428571429\n",
      "Epoch 13/600, Batch 13/13       \n",
      " testing...\n",
      "0.6071428571428571\n",
      "Epoch 14/600, Batch 13/13       \n",
      " testing...\n",
      "0.5071428571428571\n",
      "Epoch 15/600, Batch 13/13       \n",
      " testing...\n",
      "0.5142857142857142\n",
      "Epoch 16/600, Batch 13/13       \n",
      " testing...\n",
      "0.5\n",
      "Epoch 17/600, Batch 13/13       \n",
      " testing...\n",
      "0.55\n",
      "Epoch 18/600, Batch 13/13       \n",
      " testing...\n",
      "0.5571428571428572\n",
      "Epoch 19/600, Batch 13/13       \n",
      " testing...\n",
      "0.55\n",
      "Epoch 20/600, Batch 13/13       \n",
      " testing...\n",
      "0.6142857142857143\n",
      "Epoch 21/600, Batch 13/13       \n",
      " testing...\n",
      "0.6714285714285714\n",
      "Epoch 22/600, Batch 13/13       \n",
      " testing...\n",
      "0.5785714285714286\n",
      "Epoch 23/600, Batch 13/13       \n",
      " testing...\n",
      "0.5642857142857143\n",
      "Epoch 24/600, Batch 13/13       \n",
      " testing...\n",
      "0.6571428571428571\n",
      "Epoch 25/600, Batch 13/13       \n",
      " testing...\n",
      "0.5571428571428572\n",
      "Epoch 26/600, Batch 13/13       \n",
      " testing...\n",
      "0.6857142857142857\n",
      "Epoch 27/600, Batch 13/13       \n",
      " testing...\n",
      "0.6571428571428571\n",
      "Epoch 28/600, Batch 13/13       \n",
      " testing...\n",
      "0.7214285714285714\n",
      "Epoch 29/600, Batch 13/13       \n",
      " testing...\n",
      "0.6857142857142857\n",
      "Epoch 30/600, Batch 13/13       \n",
      " testing...\n",
      "0.6642857142857143\n",
      "Epoch 31/600, Batch 13/13       \n",
      " testing...\n",
      "0.7142857142857143\n",
      "Epoch 32/600, Batch 13/13       \n",
      " testing...\n",
      "0.7142857142857143\n",
      "Epoch 33/600, Batch 13/13       \n",
      " testing...\n",
      "0.7857142857142857\n",
      "Epoch 34/600, Batch 13/13       \n",
      " testing...\n",
      "0.6714285714285714\n",
      "Epoch 35/600, Batch 13/13       \n",
      " testing...\n",
      "0.7285714285714285\n",
      "Epoch 36/600, Batch 13/13       \n",
      " testing...\n",
      "0.7142857142857143\n",
      "Epoch 37/600, Batch 13/13       \n",
      " testing...\n",
      "0.6285714285714286\n",
      "Epoch 38/600, Batch 13/13       \n",
      " testing...\n",
      "0.6642857142857143\n",
      "Epoch 39/600, Batch 13/13       \n",
      " testing...\n",
      "0.6214285714285714\n",
      "Epoch 40/600, Batch 13/13       \n",
      " testing...\n",
      "0.7357142857142858\n",
      "Epoch 41/600, Batch 13/13       \n",
      " testing...\n",
      "0.7214285714285714\n",
      "Epoch 42/600, Batch 13/13       \n",
      " testing...\n",
      "0.7714285714285715\n",
      "Epoch 43/600, Batch 13/13       \n",
      " testing...\n",
      "0.6642857142857143\n",
      "Epoch 44/600, Batch 13/13       \n",
      " testing...\n",
      "0.6642857142857143\n",
      "Epoch 45/600, Batch 13/13       \n",
      " testing...\n",
      "0.7571428571428571\n",
      "Epoch 46/600, Batch 13/13       \n",
      " testing...\n",
      "0.7285714285714285\n",
      "Epoch 47/600, Batch 13/13       \n",
      " testing...\n",
      "0.7071428571428572\n",
      "Epoch 48/600, Batch 13/13       \n",
      " testing...\n",
      "0.7571428571428571\n",
      "Epoch 49/600, Batch 13/13       \n",
      " testing...\n",
      "0.7714285714285715\n",
      "Epoch 50/600, Batch 13/13       \n",
      " testing...\n",
      "0.7857142857142857\n",
      "Epoch 51/600, Batch 13/13       \n",
      " testing...\n",
      "0.7714285714285715\n",
      "Epoch 52/600, Batch 13/13       \n",
      " testing...\n",
      "0.6714285714285714\n",
      "Epoch 53/600, Batch 13/13       \n",
      " testing...\n",
      "0.7857142857142857\n",
      "Epoch 54/600, Batch 13/13       \n",
      " testing...\n",
      "0.6357142857142857\n",
      "Epoch 55/600, Batch 13/13       \n",
      " testing...\n",
      "0.7857142857142857\n",
      "Epoch 56/600, Batch 13/13       \n",
      " testing...\n",
      "0.7857142857142857\n",
      "Epoch 57/600, Batch 13/13       \n",
      " testing...\n",
      "0.7928571428571428\n",
      "Epoch 58/600, Batch 13/13       \n",
      " testing...\n",
      "0.8142857142857143\n",
      "Epoch 59/600, Batch 13/13       \n",
      " testing...\n",
      "0.7428571428571429\n",
      "Epoch 60/600, Batch 13/13       \n",
      " testing...\n",
      "0.7928571428571428\n",
      "Epoch 61/600, Batch 13/13       \n",
      " testing...\n",
      "0.75\n",
      "Epoch 62/600, Batch 13/13       \n",
      " testing...\n",
      "0.8142857142857143\n",
      "Epoch 63/600, Batch 13/13       \n",
      " testing...\n",
      "0.8142857142857143\n",
      "Epoch 64/600, Batch 13/13       \n",
      " testing...\n",
      "0.7928571428571428\n",
      "Epoch 65/600, Batch 13/13       \n",
      " testing...\n",
      "0.6928571428571428\n",
      "Epoch 66/600, Batch 13/13       \n",
      " testing...\n",
      "0.7285714285714285\n",
      "Epoch 67/600, Batch 13/13       \n",
      " testing...\n",
      "0.7928571428571428\n",
      "Epoch 68/600, Batch 13/13       \n",
      " testing...\n",
      "0.7785714285714286\n",
      "Epoch 69/600, Batch 13/13       \n",
      " testing...\n",
      "0.7714285714285715\n",
      "Epoch 70/600, Batch 13/13       \n",
      " testing...\n",
      "0.7928571428571428\n",
      "Epoch 71/600, Batch 13/13       \n",
      " testing...\n",
      "0.7928571428571428\n",
      "Epoch 72/600, Batch 13/13       \n",
      " testing...\n",
      "0.7857142857142857\n",
      "Epoch 73/600, Batch 13/13       \n",
      " testing...\n",
      "0.7357142857142858\n",
      "Epoch 74/600, Batch 13/13       \n",
      " testing...\n",
      "0.85\n",
      "Epoch 75/600, Batch 13/13       \n",
      " testing...\n",
      "0.8142857142857143\n",
      "Epoch 76/600, Batch 13/13       \n",
      " testing...\n",
      "0.8285714285714286\n",
      "Epoch 77/600, Batch 13/13       \n",
      " testing...\n",
      "0.7428571428571429\n",
      "Epoch 78/600, Batch 13/13       \n",
      " testing...\n",
      "0.8142857142857143\n",
      "Epoch 79/600, Batch 13/13       \n",
      " testing...\n",
      "0.8285714285714286\n",
      "Epoch 80/600, Batch 13/13             \n",
      " testing...\n",
      "0.7142857142857143\n",
      "Epoch 81/600, Batch 13/13       \n",
      " testing...\n",
      "0.7142857142857143\n",
      "Epoch 82/600, Batch 13/13       \n",
      " testing...\n",
      "0.7928571428571428\n",
      "Epoch 83/600, Batch 13/13       \n",
      " testing...\n",
      "0.6785714285714286\n",
      "Epoch 84/600, Batch 13/13       \n",
      " testing...\n",
      "0.7642857142857142\n",
      "Epoch 85/600, Batch 13/13       \n",
      " testing...\n",
      "0.7428571428571429\n",
      "Epoch 86/600, Batch 13/13       \n",
      " testing...\n",
      "0.8571428571428571\n",
      "Epoch 87/600, Batch 13/13       \n",
      " testing...\n",
      "0.8\n",
      "Epoch 88/600, Batch 13/13       \n",
      " testing...\n",
      "0.7714285714285715\n",
      "Epoch 89/600, Batch 13/13             \n",
      " testing...\n",
      "0.8285714285714286\n",
      "Epoch 90/600, Batch 13/13       \n",
      " testing...\n",
      "0.8142857142857143\n",
      "Epoch 91/600, Batch 13/13       \n",
      " testing...\n",
      "0.8214285714285714\n",
      "Epoch 92/600, Batch 13/13       \n",
      " testing...\n",
      "0.8\n",
      "Epoch 93/600, Batch 13/13       \n",
      " testing...\n",
      "0.7928571428571428\n",
      "Epoch 94/600, Batch 13/13       \n",
      " testing...\n",
      "0.8214285714285714\n",
      "Epoch 95/600, Batch 13/13       \n",
      " testing...\n",
      "0.8214285714285714\n",
      "Epoch 96/600, Batch 13/13       \n",
      " testing...\n",
      "0.7285714285714285\n",
      "Epoch 97/600, Batch 13/13       \n",
      " testing...\n",
      "0.85\n",
      "Epoch 98/600, Batch 13/13       \n",
      " testing...\n",
      "0.7785714285714286\n",
      "Epoch 99/600, Batch 13/13       \n",
      " testing...\n",
      "0.8285714285714286\n",
      "Epoch 100/600, Batch 13/13       \n",
      " testing...\n",
      "0.8928571428571429\n",
      "Epoch 101/600, Batch 13/13       \n",
      " testing...\n",
      "0.8428571428571429\n",
      "Epoch 102/600, Batch 13/13       \n",
      " testing...\n",
      "0.8142857142857143\n",
      "Epoch 103/600, Batch 13/13       \n",
      " testing...\n",
      "0.8214285714285714\n",
      "Epoch 104/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 105/600, Batch 13/13       \n",
      " testing...\n",
      "0.8071428571428572\n",
      "Epoch 106/600, Batch 13/13       \n",
      " testing...\n",
      "0.85\n",
      "Epoch 107/600, Batch 13/13       \n",
      " testing...\n",
      "0.8642857142857143\n",
      "Epoch 108/600, Batch 13/13       \n",
      " testing...\n",
      "0.8571428571428571\n",
      "Epoch 109/600, Batch 13/13       \n",
      " testing...\n",
      "0.9\n",
      "Epoch 110/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 111/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 112/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 113/600, Batch 13/13       \n",
      " testing...\n",
      "0.8928571428571429\n",
      "Epoch 114/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 115/600, Batch 13/13       \n",
      " testing...\n",
      "0.8428571428571429\n",
      "Epoch 116/600, Batch 13/13       \n",
      " testing...\n",
      "0.85\n",
      "Epoch 117/600, Batch 13/13       \n",
      " testing...\n",
      "0.8571428571428571\n",
      "Epoch 118/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 119/600, Batch 13/13       \n",
      " testing...\n",
      "0.85\n",
      "Epoch 120/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 121/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 122/600, Batch 13/13       \n",
      " testing...\n",
      "0.8571428571428571\n",
      "Epoch 123/600, Batch 13/13       \n",
      " testing...\n",
      "0.8642857142857143\n",
      "Epoch 124/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 125/600, Batch 13/13       \n",
      " testing...\n",
      "0.8642857142857143\n",
      "Epoch 126/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 127/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 128/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 129/600, Batch 13/13       \n",
      " testing...\n",
      "0.8642857142857143\n",
      "Epoch 130/600, Batch 13/13       \n",
      " testing...\n",
      "0.8642857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 132/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 133/600, Batch 13/13       \n",
      " testing...\n",
      "0.8928571428571429\n",
      "Epoch 134/600, Batch 13/13       \n",
      " testing...\n",
      "0.8928571428571429\n",
      "Epoch 135/600, Batch 13/13       \n",
      " testing...\n",
      "0.8928571428571429\n",
      "Epoch 136/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 137/600, Batch 13/13       \n",
      " testing...\n",
      "0.8571428571428571\n",
      "Epoch 138/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 139/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 140/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 141/600, Batch 13/13       \n",
      " testing...\n",
      "0.85\n",
      "Epoch 142/600, Batch 13/13       \n",
      " testing...\n",
      "0.8285714285714286\n",
      "Epoch 143/600, Batch 13/13       \n",
      " testing...\n",
      "0.85\n",
      "Epoch 144/600, Batch 13/13       \n",
      " testing...\n",
      "0.8642857142857143\n",
      "Epoch 145/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 146/600, Batch 13/13       \n",
      " testing...\n",
      "0.8571428571428571\n",
      "Epoch 147/600, Batch 13/13       \n",
      " testing...\n",
      "0.8571428571428571\n",
      "Epoch 148/600, Batch 13/13       \n",
      " testing...\n",
      "0.85\n",
      "Epoch 149/600, Batch 13/13       \n",
      " testing...\n",
      "0.8642857142857143\n",
      "Epoch 150/600, Batch 13/13             \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 151/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 152/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 153/600, Batch 13/13             \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 154/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 155/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 156/600, Batch 13/13             \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 157/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 158/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 159/600, Batch 13/13       \n",
      " testing...\n",
      "0.8928571428571429\n",
      "Epoch 160/600, Batch 13/13       \n",
      " testing...\n",
      "0.8428571428571429\n",
      "Epoch 161/600, Batch 13/13       \n",
      " testing...\n",
      "0.8428571428571429\n",
      "Epoch 162/600, Batch 13/13       \n",
      " testing...\n",
      "0.85\n",
      "Epoch 163/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 164/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 165/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 166/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 167/600, Batch 13/13       \n",
      " testing...\n",
      "0.8571428571428571\n",
      "Epoch 168/600, Batch 13/13       \n",
      " testing...\n",
      "0.85\n",
      "Epoch 169/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 170/600, Batch 13/13       \n",
      " testing...\n",
      "0.8428571428571429\n",
      "Epoch 171/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 172/600, Batch 13/13       \n",
      " testing...\n",
      "0.8642857142857143\n",
      "Epoch 173/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 174/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 175/600, Batch 13/13             \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 176/600, Batch 13/13       \n",
      " testing...\n",
      "0.85\n",
      "Epoch 177/600, Batch 13/13       \n",
      " testing...\n",
      "0.8428571428571429\n",
      "Epoch 178/600, Batch 13/13       \n",
      " testing...\n",
      "0.8642857142857143\n",
      "Epoch 179/600, Batch 13/13       \n",
      " testing...\n",
      "0.8571428571428571\n",
      "Epoch 180/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 181/600, Batch 13/13       \n",
      " testing...\n",
      "0.8428571428571429\n",
      "Epoch 182/600, Batch 13/13       \n",
      " testing...\n",
      "0.8428571428571429\n",
      "Epoch 183/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 184/600, Batch 13/13       \n",
      " testing...\n",
      "0.8285714285714286\n",
      "Epoch 185/600, Batch 13/13       \n",
      " testing...\n",
      "0.8357142857142857\n",
      "Epoch 186/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 187/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 188/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 189/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 190/600, Batch 13/13       \n",
      " testing...\n",
      "0.8571428571428571\n",
      "Epoch 191/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 192/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 193/600, Batch 13/13       \n",
      " testing...\n",
      "0.9\n",
      "Epoch 194/600, Batch 13/13       \n",
      " testing...\n",
      "0.8214285714285714\n",
      "Epoch 195/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 196/600, Batch 13/13       \n",
      " testing...\n",
      "0.8428571428571429\n",
      "Epoch 197/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 198/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 199/600, Batch 13/13       \n",
      " testing...\n",
      "0.8571428571428571\n",
      "Epoch 200/600, Batch 13/13       \n",
      " testing...\n",
      "0.8642857142857143\n",
      "Epoch 201/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 202/600, Batch 13/13       \n",
      " testing...\n",
      "0.8928571428571429\n",
      "Epoch 203/600, Batch 13/13       \n",
      " testing...\n",
      "0.9\n",
      "Epoch 204/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 205/600, Batch 13/13       \n",
      " testing...\n",
      "0.8928571428571429\n",
      "Epoch 206/600, Batch 13/13       \n",
      " testing...\n",
      "0.8928571428571429\n",
      "Epoch 207/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n",
      "Epoch 208/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 209/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 210/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 211/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 212/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 213/600, Batch 13/13       \n",
      " testing...\n",
      "0.8714285714285714\n",
      "Epoch 214/600, Batch 13/13       \n",
      " testing...\n",
      "0.8928571428571429\n",
      "Epoch 215/600, Batch 13/13       \n",
      " testing...\n",
      "0.8785714285714286\n",
      "Epoch 216/600, Batch 13/13       \n",
      " testing...\n",
      "0.8928571428571429\n",
      "Epoch 217/600, Batch 13/13       \n",
      " testing...\n",
      "0.9\n",
      "Epoch 218/600, Batch 13/13       \n",
      " testing...\n",
      "0.8857142857142857\n"
     ]
    }
   ],
   "source": [
    "model = Mpeg7_model()\n",
    "model.center_init([mpeg7_data_set[i] for i in train_sampler])\n",
    "model.cuda()\n",
    "\n",
    "stats = defaultdict(list)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=train_env.lr_initial, momentum=train_env.momentum)\n",
    "\n",
    "for i_epoch in range(1, train_env.n_epochs+1):      \n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    epoch_loss = 0    \n",
    "    \n",
    "    if i_epoch % train_env.lr_epoch_step == 0:\n",
    "        adapt_lr(opt, lambda lr: lr*0.5)\n",
    "    \n",
    "    for i_batch, (x, y) in enumerate(dl_train, 1):              \n",
    "        \n",
    "        y = torch.autograd.Variable(y)\n",
    "        \n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            y_hat = model(x)            \n",
    "            loss = nn.functional.cross_entropy(y_hat, y)   \n",
    "            loss.backward()\n",
    "            \n",
    "#             torch.nn.utils.clip_grad_norm(model.parameters_split()['non_linear'], 1, norm_type='inf')\n",
    "            return loss\n",
    "\n",
    "        loss = opt.step(closure)\n",
    "        \n",
    "        epoch_loss += float(loss)\n",
    "        stats['loss_by_batch'].append(float(loss))\n",
    "        stats['centers'].append(model.slayers['dim_0_dir_0'].centers.data.cpu().numpy())\n",
    "        \n",
    "        print(\"Epoch {}/{}, Batch {}/{}\".format(i_epoch, train_env.n_epochs, i_batch, len(dl_train)), end=\"       \\r\")\n",
    "  \n",
    "    stats['train_loss_by_epoch'].append(epoch_loss/len(dl_train))\n",
    "        \n",
    "    print(\"\\n\\r testing...\")\n",
    "    model.eval()    \n",
    "    true_samples = 0\n",
    "    seen_samples = 0\n",
    "    epoch_test_loss = 0\n",
    "    for i_batch, (x, y) in enumerate(dl_test):\n",
    "\n",
    "        y_hat = model(x)\n",
    "        epoch_test_loss += float(nn.functional.cross_entropy(y_hat, torch.autograd.Variable(y.cuda())).data)\n",
    "\n",
    "        y_hat = y_hat.max(dim=1)[1].data.long()\n",
    "\n",
    "        true_samples += (y_hat == y).sum()\n",
    "        seen_samples += y.size(0)  \n",
    "     \n",
    "    stats['test_accuracy'].append(true_samples/seen_samples)\n",
    "    stats['test_loss_by_epoch'].append(epoch_test_loss/len(dl_test))\n",
    "    print(true_samples/seen_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "if 'centers' in stats:\n",
    "    c_start = stats['centers'][0]\n",
    "    c_end = stats['centers'][-1]\n",
    "\n",
    "    plt.plot(c_start[:,0], c_start[:, 1], 'bo', label='center initialization')\n",
    "    plt.plot(c_end[:,0], c_end[:, 1], 'ro', label='center learned')\n",
    "\n",
    "    all_centers = numpy.stack(stats['centers'], axis=0)\n",
    "    for i in range(all_centers.shape[1]):\n",
    "        points = all_centers[:,i, :]\n",
    "        plt.plot(points[:, 0], points[:, 1], '-k', alpha=0.25)\n",
    "        \n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(stats['train_loss_by_epoch'], label='train_loss')\n",
    "plt.plot(stats['test_loss_by_epoch'], label='test_loss')\n",
    "plt.plot(stats['test_accuracy'], label='test_accuracy')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['test_accuracy'][-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.slayers['dim_0_dir_0'].radius.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
