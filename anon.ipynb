{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch    \n",
    "torch.manual_seed(123)\n",
    "import random\n",
    "random.seed(123)\n",
    "# import torch.multiprocessing\n",
    "# torch.multiprocessing.set_start_method(\"spawn\")\n",
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "from chofer_torchex.utils.data.collate import dict_sample_target_iter_concat\n",
    "from chofer_torchex.utils.functional import collection_cascade, cuda_cascade\n",
    "from chofer_tda_datasets import AnonEigenvaluePredict\n",
    "from chofer_tda_datasets.transforms import Hdf5GroupToDict\n",
    "from jmlr_2018_code.utils import *\n",
    "from chofer_torchex.nn.slayer import SLayerRationalHat, LinearRationalStretchedBirthLifeTimeCoordinateTransform, prepare_batch\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(2)\n",
    "\n",
    "\n",
    "class AnonCollate:   \n",
    "    def __init__(self, cuda=True):\n",
    "        self.cuda = cuda\n",
    "        \n",
    "    def __call__(self, sample_target_iter):\n",
    "        x, y = [], []\n",
    "        for x_i, y_i in sample_target_iter:\n",
    "            x.append(x_i)\n",
    "            y.append(y_i)\n",
    "\n",
    "        x = prepare_batch(x, 2)            \n",
    "        y = torch.Tensor(y)\n",
    "\n",
    "        if self.cuda:\n",
    "            # Shifting the necessary parts of the prepared batch to the cuda\n",
    "            x = (x[0].cuda(), x[1].cuda(), x[2], x[3])\n",
    "            y = y.cuda()\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "class train_env:\n",
    "    n_epochs = 200\n",
    "    lr_initial = 0.01\n",
    "    lr_epoch_step = 20\n",
    "    batch_size = 64\n",
    "    train_size = 0.9\n",
    "    nu = 0.01\n",
    "    n_target_bins = 100\n",
    "\n",
    "    \n",
    "dataset = AnonEigenvaluePredict(data_root_folder_path='/scratch1/chofer/jmlr2018_data/')\n",
    "dataset.keys_essential = ('dim_0_ess', 'dim_1_ess')\n",
    "dataset.keys_not_essential = ('dim_0',)\n",
    "\n",
    "\n",
    "coordinate_transform  = LinearRationalStretchedBirthLifeTimeCoordinateTransform(nu=train_env.nu)    \n",
    "    \n",
    "    \n",
    "def normalize(x):\n",
    "    c = x[:, 1].max()    \n",
    "    return x/c\n",
    "\n",
    "        \n",
    "dataset.data_transforms = [lambda x: x['dim_0'].value,\n",
    "                           lambda x: torch.from_numpy(x).float(), \n",
    "                           coordinate_transform,\n",
    "                           normalize\n",
    "                           ]\n",
    "def histogramize(x):\n",
    "    return np.histogram(x, normed=True, bins=train_env.n_target_bins,range=(0,2))[0].tolist()\n",
    "    \n",
    "\n",
    "dataset.target_transforms = [histogramize]                      \n",
    "reddit_collate = AnonCollate(cuda=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LinearCell(n_in, n_out):\n",
    "    m = nn.Sequential(nn.Linear(n_in, n_out), \n",
    "                      nn.BatchNorm1d(n_out), \n",
    "                      nn.ReLU())\n",
    "    m.out_features = m[0].out_features\n",
    "    return m\n",
    "\n",
    "\n",
    "def Slayer(n_elements, point_dim):\n",
    "    return SLayerRationalHat(n_elements, point_dimension=2, radius_init=1)   \n",
    "\n",
    "\n",
    "class AnonModel(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super().__init__()  \n",
    "        \n",
    "        dim_0_n_elements = 200        \n",
    "        self.dim_0 = Slayer(dim_0_n_elements, 2)          \n",
    "\n",
    "        self.regressor = nn.Sequential(  \n",
    "                                         nn.BatchNorm1d(dim_0_n_elements),\n",
    "                                         LinearCell(dim_0_n_elements, train_env.n_target_bins),\n",
    "#                                        nn.Dropout(0.2),\n",
    "#                                        LinearCell(cls_in_size, int(cls_in_size/2)),                                        \n",
    "                                         nn.Linear(train_env.n_target_bins, train_env.n_target_bins))\n",
    "                         \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.dim_0(x)\n",
    "        x = self.regressor(x)        \n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def centers_init(self, sample_target_iter):   \n",
    "        x = [x for x, _ in sample_target_iter]\n",
    "        dim_0 = torch.cat(x, dim=0)\n",
    "        dim_0 = dim_0.numpy()\n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters=self.dim_0.centers.size(0), \n",
    "                                        init='k-means++', \n",
    "                                        random_state=123, \n",
    "                                        n_jobs=10, \n",
    "                                        n_init=2, )                           \n",
    "        kmeans.fit(dim_0)\n",
    "        centers = kmeans.cluster_centers_\n",
    "        centers = torch.from_numpy(centers)\n",
    "        self.dim_0.centers.data = centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 0\n",
      "Epoch 1/200, Batch 29/44       \r"
     ]
    }
   ],
   "source": [
    "stats_of_runs = []\n",
    "def experiment():      \n",
    "    splitter = ShuffleSplit(n_splits=1, \n",
    "                            train_size=train_env.train_size, \n",
    "                            test_size=1-train_env.train_size, \n",
    "                            random_state=123)\n",
    "    \n",
    "    train_test_splits = list(splitter.split(X=dataset.targets, y=dataset.targets))\n",
    "    train_test_splits = [(train_i.tolist(), test_i.tolist()) for train_i, test_i in train_test_splits]\n",
    "    \n",
    "    for run_i, (train_i, test_i) in enumerate(train_test_splits):\n",
    "        print('')\n",
    "        print('Run', run_i)\n",
    "        \n",
    "        model = AnonModel()\n",
    "        model.centers_init([dataset[i] for i in train_i])\n",
    "        model.cuda()\n",
    "\n",
    "        stats = defaultdict(list)\n",
    "        stats_of_runs.append(stats)\n",
    "        \n",
    "        opt=torch.optim.SGD(model.parameters(), lr=train_env.lr_initial, momentum=0.9)\n",
    "\n",
    "        for i_epoch in range(1, train_env.n_epochs+1):      \n",
    "\n",
    "            model.train()\n",
    "\n",
    "            train_sampler = [i for i in train_i] \n",
    "            random.shuffle(train_sampler)\n",
    "            \n",
    "            dl_train = DataLoader(dataset,\n",
    "                              batch_size=train_env.batch_size, \n",
    "                              collate_fn=reddit_collate,\n",
    "                              sampler=train_sampler, )\n",
    "\n",
    "            dl_test = DataLoader(dataset,\n",
    "                                 batch_size=train_env.batch_size, \n",
    "                                 collate_fn=reddit_collate, \n",
    "                                 sampler=test_i,)\n",
    "\n",
    "            epoch_loss = 0\n",
    "\n",
    "            if i_epoch % train_env.lr_epoch_step == 0:\n",
    "                adapt_lr(opt, lambda lr: lr*0.5)\n",
    "\n",
    "            for i_batch, (x, y) in enumerate(dl_train, 1):  \n",
    "                \n",
    "                y = torch.autograd.Variable(y)\n",
    "\n",
    "                def closure():\n",
    "                    opt.zero_grad()\n",
    "                    y_hat = model(x)            \n",
    "                    loss = nn.functional.mse_loss(y_hat, y)   \n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "\n",
    "                loss = opt.step(closure)\n",
    "\n",
    "                epoch_loss += float(loss)\n",
    "                stats['loss_by_batch'].append(float(loss))\n",
    "                stats['centers'].append(model.dim_0.centers.data.cpu().numpy())\n",
    "\n",
    "                print(\"Epoch {}/{}, Batch {}/{}\".format(i_epoch, train_env.n_epochs, i_batch, len(dl_train)), end=\"       \\r\")\n",
    "\n",
    "            stats['train_loss_by_epoch'].append(epoch_loss/len(dl_train))\n",
    "\n",
    "            print(\"\\n\\r testing...\")\n",
    "            model.eval()    \n",
    "            true_samples = 0\n",
    "            seen_samples = 0\n",
    "            epoch_test_loss = 0\n",
    "            for i_batch, (x, y) in enumerate(dl_test):\n",
    "                y_hat = model(x)\n",
    "                epoch_test_loss += float(nn.functional.mse_loss(y_hat, torch.autograd.Variable(y.cuda())).data) \n",
    "                seen_samples += y.size(0)\n",
    "                \n",
    "            avg_epoch_test_loss = epoch_test_loss/len(dl_test)   \n",
    "            stats['test_loss_by_epoch'].append(avg_epoch_test_loss)            \n",
    "            print(avg_epoch_test_loss)\n",
    "        \n",
    "        stats['train_i'] = train_i\n",
    "        stats['test_i'] = test_i\n",
    "        stats['model'] = model.cpu()\n",
    "        \n",
    "experiment()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "stats = stats_of_runs[0]\n",
    "c_start = stats['centers'][0]\n",
    "c_end = stats['centers'][-1]\n",
    "\n",
    "plt.plot(c_start[:,0], c_start[:, 1], 'bo', label='center initialization')\n",
    "plt.plot(c_end[:,0], c_end[:, 1], 'ro', label='center learned')\n",
    "\n",
    "all_centers = numpy.stack(stats['centers'], axis=0)\n",
    "for i in range(all_centers.shape[1]):\n",
    "    points = all_centers[:,i, :]\n",
    "    plt.plot(points[:, 0], points[:, 1], '-k')\n",
    "    \n",
    "plt.legend()\n",
    "\n",
    "# for c_i_start, c_i_end in zip(c_start, c_end):\n",
    "#     points = numpy.stack([c_i_start, c_i_end], axis=0)\n",
    "#     plt.plot(points[:,0], points[:,1], '-k')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(stats['train_loss_by_epoch'], label='train_loss')\n",
    "plt.plot(stats['test_loss_by_epoch'], label='test_loss')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = stats_of_runs[0]\n",
    "test_i = stats['test_i']\n",
    "\n",
    "dl_test = DataLoader(dataset,\n",
    "                     batch_size=train_env.batch_size, \n",
    "                     collate_fn=reddit_collate, \n",
    "                     sampler=test_i,)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "model=stats['model']\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "for i_batch, (x, y_true_i) in enumerate(dl_test):\n",
    "    y_pred_i = model(x)\n",
    "    y_true += [a for a in y_true_i.cpu().numpy()]\n",
    "    y_pred += [a for a in y_pred_i.data.cpu().numpy()]\n",
    "    \n",
    "for i, (y_true_i, y_pred_i) in enumerate(zip(y_true, y_pred)):\n",
    "    fig = plt.figure()\n",
    "    x = np.linspace(0, 2, train_env.n_target_bins)\n",
    "    plt.plot(x, y_true_i, 'go', label='true',)\n",
    "    plt.plot(x, y_pred_i, 'bo', label='pred')\n",
    "    plt.legend()\n",
    "    plt.savefig('./anon_images/' + str(i) + '.png')\n",
    "    \n",
    "plt.show()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(5, 10)\n",
    "print(x)\n",
    "[t for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
