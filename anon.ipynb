{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch    \n",
    "torch.manual_seed(123)\n",
    "import random\n",
    "random.seed(123)\n",
    "# import torch.multiprocessing\n",
    "# torch.multiprocessing.set_start_method(\"spawn\")\n",
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "from chofer_torchex.utils.data.collate import dict_sample_target_iter_concat\n",
    "from chofer_torchex.utils.functional import collection_cascade, cuda_cascade\n",
    "from chofer_tda_datasets import AnonEigenvaluePredict\n",
    "from chofer_tda_datasets.transforms import Hdf5GroupToDict\n",
    "from jmlr_2018_code.utils import *\n",
    "from chofer_torchex.nn.slayer import SLayerRationalHat, LinearRationalStretchedBirthLifeTimeCoordinateTransform, prepare_batch\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(3)\n",
    "\n",
    "\n",
    "class AnonCollate:   \n",
    "    def __init__(self, dataset, cuda=True):\n",
    "        self.cuda = cuda\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __call__(self, sample_target_iter):\n",
    "        x, y = dict_sample_target_iter_concat(sample_target_iter)\n",
    "\n",
    "        for k in self.dataset.keys_not_essential:\n",
    "            batch_view = x[k]\n",
    "            x[k] = prepare_batch(batch_view, 2)\n",
    "            \n",
    "        for k in self.dataset.keys_essential:\n",
    "            batch_view = x[k]\n",
    "            x[k] = prepare_batch(batch_view, 1)   \n",
    "            \n",
    "        y = torch.Tensor(y)\n",
    "\n",
    "        if self.cuda:\n",
    "            # Shifting the necessary parts of the prepared batch to the cuda\n",
    "            x = {k: collection_cascade(v,\n",
    "                                       lambda x: isinstance(x, tuple),\n",
    "                                       lambda x: (x[0].cuda(), x[1].cuda(), x[2], x[3]))\n",
    "                 for k, v in x.items()}\n",
    "\n",
    "            y = y.cuda()\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "class train_env:\n",
    "    n_epochs = 100\n",
    "    lr_initial = 0.01\n",
    "    lr_epoch_step = 20\n",
    "    batch_size = 5\n",
    "    train_size = 0.9\n",
    "    nu = 0.01\n",
    "    n_target_bins = 100\n",
    "\n",
    "    \n",
    "dataset = AnonEigenvaluePredict(data_root_folder_path='/scratch1/chofer/jmlr2018_data/')\n",
    "dataset.keys_essential = ('dim_0_ess', 'dim_1_ess')\n",
    "dataset.keys_not_essential = ('dim_0',)\n",
    "\n",
    "def coordinate_transform(x):\n",
    "    t = LinearRationalStretchedBirthLifeTimeCoordinateTransform(nu=train_env.nu)    \n",
    "    for k in dataset.keys_not_essential:\n",
    "        x[k] = t(x[k])   \n",
    "        \n",
    "    return x\n",
    "    \n",
    "def normalize(x):\n",
    "    dim_1 =  x['dim_0']/x['dim_0'][:, 1].max() \n",
    "    dim_1 = dim_1[(dim_1[:,1] - dim_1[:,0] > train_env.nu).nonzero().squeeze()]\n",
    "    \n",
    "    x['dim_0'] = dim_1    \n",
    "    \n",
    "    x['dim_0_ess'] = (x['dim_0_ess']/x['dim_0_ess'].max()).unsqueeze(1)\n",
    "    x['dim_1_ess'] = (x['dim_1_ess']/x['dim_1_ess'].max()).unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "        \n",
    "dataset.data_transforms = [Hdf5GroupToDict(),\n",
    "                           numpy_to_torch_cascade,\n",
    "                           coordinate_transform,\n",
    "                           normalize\n",
    "                           ]\n",
    "def histogramize(x):\n",
    "    return np.histogram(x, normed=True, bins=train_env.n_target_bins,range=(0,2))[0].tolist()\n",
    "    \n",
    "\n",
    "dataset.target_transforms = [histogramize]                      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fc125fda3f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pma/chofer/repositories/chofer_tda_datasets/chofer_tda_datasets/utils/h5py_dataset.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pma/chofer/repositories/chofer_tda_datasets/chofer_tda_datasets/utils/h5py_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_target_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pma/chofer/repositories/chofer_tda_datasets/chofer_tda_datasets/utils/h5py_dataset.py\u001b[0m in \u001b[0;36m_get_target_i\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_target_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_h5py_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_hdf5_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lengths = defaultdict(list)\n",
    "for x, y in dataset:\n",
    "    for k, v in x.items():\n",
    "        lengths[k].append(v.size(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'dim_0': [1610,\n",
       "              2577,\n",
       "              2419,\n",
       "              2526,\n",
       "              2661,\n",
       "              1933,\n",
       "              1786,\n",
       "              3380,\n",
       "              1885,\n",
       "              1594,\n",
       "              1631,\n",
       "              1706,\n",
       "              1944,\n",
       "              1362,\n",
       "              2392,\n",
       "              1519,\n",
       "              1723,\n",
       "              2014,\n",
       "              1840,\n",
       "              1713,\n",
       "              1777,\n",
       "              1842,\n",
       "              1845,\n",
       "              2051,\n",
       "              1426,\n",
       "              1130,\n",
       "              1700,\n",
       "              1502,\n",
       "              1813,\n",
       "              1732,\n",
       "              2327,\n",
       "              1681,\n",
       "              1556,\n",
       "              2741,\n",
       "              1720,\n",
       "              2037,\n",
       "              2522,\n",
       "              2585,\n",
       "              1901,\n",
       "              2091,\n",
       "              1875,\n",
       "              1882,\n",
       "              1761,\n",
       "              1345,\n",
       "              2078,\n",
       "              2002,\n",
       "              1299,\n",
       "              1660,\n",
       "              1665,\n",
       "              2454,\n",
       "              2506,\n",
       "              2531,\n",
       "              1732,\n",
       "              1936,\n",
       "              2246,\n",
       "              2430,\n",
       "              1801,\n",
       "              1736,\n",
       "              2689,\n",
       "              1531,\n",
       "              1744,\n",
       "              1869,\n",
       "              2217,\n",
       "              1810,\n",
       "              1688,\n",
       "              1572,\n",
       "              2480,\n",
       "              1851,\n",
       "              1954,\n",
       "              1788,\n",
       "              2297,\n",
       "              2026,\n",
       "              1913,\n",
       "              1916,\n",
       "              1927,\n",
       "              1903,\n",
       "              1973,\n",
       "              1807,\n",
       "              2351,\n",
       "              1973,\n",
       "              1994,\n",
       "              2161,\n",
       "              1368,\n",
       "              1830,\n",
       "              1894,\n",
       "              2378,\n",
       "              2641,\n",
       "              1371,\n",
       "              2323,\n",
       "              2145,\n",
       "              828,\n",
       "              1193,\n",
       "              2436,\n",
       "              1894,\n",
       "              2325,\n",
       "              1807,\n",
       "              2012,\n",
       "              3226,\n",
       "              1743,\n",
       "              1890,\n",
       "              1757,\n",
       "              2389,\n",
       "              2076,\n",
       "              2497,\n",
       "              2259,\n",
       "              1536,\n",
       "              1670,\n",
       "              1469,\n",
       "              2604,\n",
       "              2431,\n",
       "              1946,\n",
       "              2977,\n",
       "              1605,\n",
       "              1882,\n",
       "              2184,\n",
       "              1723,\n",
       "              2746,\n",
       "              1923,\n",
       "              2237,\n",
       "              2645,\n",
       "              2064,\n",
       "              1653,\n",
       "              823,\n",
       "              2543,\n",
       "              2417,\n",
       "              1969,\n",
       "              1912,\n",
       "              1483,\n",
       "              1540,\n",
       "              1361,\n",
       "              2279,\n",
       "              2172,\n",
       "              2556,\n",
       "              1650,\n",
       "              1967,\n",
       "              1442,\n",
       "              1967,\n",
       "              2221,\n",
       "              2241,\n",
       "              2762,\n",
       "              1842,\n",
       "              2434,\n",
       "              2351,\n",
       "              2286,\n",
       "              2567,\n",
       "              1976,\n",
       "              2397,\n",
       "              2283,\n",
       "              3261,\n",
       "              2273,\n",
       "              1705,\n",
       "              1932,\n",
       "              2343,\n",
       "              1985,\n",
       "              1638,\n",
       "              1857,\n",
       "              2254,\n",
       "              1854,\n",
       "              2079,\n",
       "              2367,\n",
       "              1671,\n",
       "              2013,\n",
       "              2066,\n",
       "              1640,\n",
       "              1509,\n",
       "              1767,\n",
       "              2045,\n",
       "              1311,\n",
       "              1848,\n",
       "              1692,\n",
       "              1496,\n",
       "              1622,\n",
       "              1665,\n",
       "              1539,\n",
       "              1660,\n",
       "              1521,\n",
       "              2321,\n",
       "              1873,\n",
       "              1777,\n",
       "              2783,\n",
       "              3325,\n",
       "              2511,\n",
       "              2050,\n",
       "              1766,\n",
       "              1698,\n",
       "              2364,\n",
       "              1926,\n",
       "              1654,\n",
       "              1402,\n",
       "              3075,\n",
       "              2204,\n",
       "              1995,\n",
       "              1948,\n",
       "              1916,\n",
       "              2349,\n",
       "              1587,\n",
       "              1839,\n",
       "              1511,\n",
       "              1901,\n",
       "              1673,\n",
       "              1784,\n",
       "              1531,\n",
       "              1332,\n",
       "              1331,\n",
       "              1206,\n",
       "              2426,\n",
       "              2450,\n",
       "              2436,\n",
       "              2412,\n",
       "              1230,\n",
       "              2075,\n",
       "              1871,\n",
       "              1683,\n",
       "              1850,\n",
       "              1828,\n",
       "              1929,\n",
       "              2271,\n",
       "              1369,\n",
       "              1707,\n",
       "              1333,\n",
       "              1346,\n",
       "              1953,\n",
       "              1852,\n",
       "              2747,\n",
       "              2141,\n",
       "              1453,\n",
       "              2017,\n",
       "              1915,\n",
       "              2034,\n",
       "              1694,\n",
       "              2447,\n",
       "              1941,\n",
       "              1217,\n",
       "              1885,\n",
       "              2000,\n",
       "              2463,\n",
       "              2491,\n",
       "              1550,\n",
       "              1243,\n",
       "              1847,\n",
       "              2448,\n",
       "              1645,\n",
       "              1608,\n",
       "              1811,\n",
       "              2057,\n",
       "              1951,\n",
       "              1922,\n",
       "              2141,\n",
       "              1502,\n",
       "              2146,\n",
       "              1675,\n",
       "              1748,\n",
       "              1672,\n",
       "              2337,\n",
       "              1709,\n",
       "              1722,\n",
       "              2259,\n",
       "              2553,\n",
       "              1911,\n",
       "              1954,\n",
       "              1971,\n",
       "              1893,\n",
       "              2605,\n",
       "              1883,\n",
       "              1868,\n",
       "              1373,\n",
       "              1640,\n",
       "              1753,\n",
       "              2036,\n",
       "              1578,\n",
       "              1839,\n",
       "              2264,\n",
       "              2382,\n",
       "              1523,\n",
       "              2282,\n",
       "              2632,\n",
       "              1765,\n",
       "              1920,\n",
       "              1978,\n",
       "              1778,\n",
       "              1243,\n",
       "              1791,\n",
       "              2121,\n",
       "              2439,\n",
       "              2667,\n",
       "              1791,\n",
       "              1514,\n",
       "              1351,\n",
       "              1477,\n",
       "              1695,\n",
       "              1727,\n",
       "              2368,\n",
       "              2205,\n",
       "              1649,\n",
       "              1611,\n",
       "              2191,\n",
       "              2131,\n",
       "              2004,\n",
       "              1491,\n",
       "              2184,\n",
       "              1330,\n",
       "              1666,\n",
       "              1243,\n",
       "              1553,\n",
       "              1698,\n",
       "              1854,\n",
       "              1995,\n",
       "              1766,\n",
       "              2388,\n",
       "              2626,\n",
       "              2034,\n",
       "              2324,\n",
       "              2491,\n",
       "              1827,\n",
       "              2195,\n",
       "              2081,\n",
       "              2019,\n",
       "              1619,\n",
       "              1857,\n",
       "              2359,\n",
       "              1429,\n",
       "              3060,\n",
       "              1002,\n",
       "              1664,\n",
       "              2158,\n",
       "              2170,\n",
       "              1285,\n",
       "              2603,\n",
       "              1651,\n",
       "              2328,\n",
       "              2439,\n",
       "              2413,\n",
       "              1932,\n",
       "              2814,\n",
       "              1899,\n",
       "              1675,\n",
       "              1899,\n",
       "              1289,\n",
       "              1903,\n",
       "              2206,\n",
       "              2019,\n",
       "              1660,\n",
       "              2062,\n",
       "              2036,\n",
       "              1861,\n",
       "              1499,\n",
       "              1847,\n",
       "              2383,\n",
       "              2564,\n",
       "              2231,\n",
       "              2154,\n",
       "              1700,\n",
       "              1985,\n",
       "              1911,\n",
       "              1723,\n",
       "              2114,\n",
       "              1753,\n",
       "              1552,\n",
       "              1277,\n",
       "              1497,\n",
       "              1726,\n",
       "              1949,\n",
       "              1253,\n",
       "              1715,\n",
       "              1748,\n",
       "              2827,\n",
       "              2231,\n",
       "              2371,\n",
       "              3037,\n",
       "              2239,\n",
       "              1546,\n",
       "              2364,\n",
       "              2204,\n",
       "              2286,\n",
       "              1984,\n",
       "              2514,\n",
       "              2912,\n",
       "              1915,\n",
       "              2265,\n",
       "              1190,\n",
       "              2525,\n",
       "              2598,\n",
       "              2476,\n",
       "              2394,\n",
       "              1628,\n",
       "              1834,\n",
       "              2188,\n",
       "              1873,\n",
       "              1243,\n",
       "              1812,\n",
       "              1666,\n",
       "              1962,\n",
       "              2044,\n",
       "              2855,\n",
       "              2028,\n",
       "              1356],\n",
       "             'dim_0_ess': [1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              1],\n",
       "             'dim_1_ess': [96505,\n",
       "              52618,\n",
       "              31149,\n",
       "              43954,\n",
       "              26236,\n",
       "              80686,\n",
       "              66044,\n",
       "              27747,\n",
       "              98055,\n",
       "              116450,\n",
       "              89834,\n",
       "              118900,\n",
       "              90845,\n",
       "              88864,\n",
       "              48916,\n",
       "              97401,\n",
       "              92825,\n",
       "              85033,\n",
       "              70758,\n",
       "              93719,\n",
       "              63010,\n",
       "              94086,\n",
       "              134163,\n",
       "              112765,\n",
       "              103233,\n",
       "              114821,\n",
       "              105063,\n",
       "              86462,\n",
       "              81273,\n",
       "              80052,\n",
       "              87706,\n",
       "              79545,\n",
       "              78848,\n",
       "              61023,\n",
       "              46100,\n",
       "              36340,\n",
       "              37185,\n",
       "              51700,\n",
       "              43848,\n",
       "              52716,\n",
       "              82209,\n",
       "              95492,\n",
       "              119447,\n",
       "              127927,\n",
       "              97947,\n",
       "              56094,\n",
       "              76679,\n",
       "              94875,\n",
       "              62816,\n",
       "              43679,\n",
       "              28277,\n",
       "              24615,\n",
       "              97826,\n",
       "              87247,\n",
       "              41800,\n",
       "              28645,\n",
       "              56084,\n",
       "              66837,\n",
       "              59159,\n",
       "              103431,\n",
       "              41125,\n",
       "              65139,\n",
       "              20692,\n",
       "              65064,\n",
       "              99482,\n",
       "              72383,\n",
       "              31320,\n",
       "              96453,\n",
       "              46659,\n",
       "              95208,\n",
       "              80815,\n",
       "              70101,\n",
       "              105782,\n",
       "              42779,\n",
       "              43937,\n",
       "              52402,\n",
       "              77281,\n",
       "              51363,\n",
       "              42101,\n",
       "              34905,\n",
       "              78515,\n",
       "              68701,\n",
       "              80621,\n",
       "              121055,\n",
       "              91034,\n",
       "              54279,\n",
       "              32114,\n",
       "              103773,\n",
       "              62618,\n",
       "              87346,\n",
       "              52473,\n",
       "              102693,\n",
       "              54538,\n",
       "              89215,\n",
       "              27055,\n",
       "              73749,\n",
       "              91723,\n",
       "              24833,\n",
       "              35862,\n",
       "              76303,\n",
       "              121011,\n",
       "              53537,\n",
       "              24306,\n",
       "              40847,\n",
       "              37181,\n",
       "              83604,\n",
       "              102147,\n",
       "              122843,\n",
       "              47144,\n",
       "              35434,\n",
       "              101869,\n",
       "              35911,\n",
       "              93311,\n",
       "              64079,\n",
       "              77558,\n",
       "              84437,\n",
       "              34250,\n",
       "              34219,\n",
       "              109862,\n",
       "              49386,\n",
       "              29553,\n",
       "              73031,\n",
       "              52037,\n",
       "              76534,\n",
       "              59251,\n",
       "              65381,\n",
       "              53366,\n",
       "              56785,\n",
       "              69057,\n",
       "              71699,\n",
       "              25318,\n",
       "              78601,\n",
       "              70338,\n",
       "              132669,\n",
       "              84322,\n",
       "              93504,\n",
       "              106820,\n",
       "              75968,\n",
       "              42600,\n",
       "              32803,\n",
       "              52794,\n",
       "              41438,\n",
       "              80959,\n",
       "              54608,\n",
       "              45182,\n",
       "              58077,\n",
       "              61220,\n",
       "              76530,\n",
       "              23337,\n",
       "              15516,\n",
       "              113149,\n",
       "              114341,\n",
       "              43068,\n",
       "              84774,\n",
       "              77608,\n",
       "              63749,\n",
       "              80327,\n",
       "              79859,\n",
       "              33282,\n",
       "              54861,\n",
       "              96950,\n",
       "              87392,\n",
       "              95697,\n",
       "              131121,\n",
       "              64362,\n",
       "              41991,\n",
       "              62080,\n",
       "              104123,\n",
       "              84404,\n",
       "              36698,\n",
       "              62906,\n",
       "              59408,\n",
       "              67865,\n",
       "              64538,\n",
       "              52538,\n",
       "              76737,\n",
       "              77613,\n",
       "              55625,\n",
       "              100329,\n",
       "              19577,\n",
       "              28114,\n",
       "              51536,\n",
       "              53515,\n",
       "              106150,\n",
       "              62610,\n",
       "              84601,\n",
       "              84227,\n",
       "              116300,\n",
       "              69006,\n",
       "              41597,\n",
       "              31403,\n",
       "              64475,\n",
       "              80498,\n",
       "              66315,\n",
       "              96345,\n",
       "              78141,\n",
       "              93795,\n",
       "              42569,\n",
       "              95734,\n",
       "              136073,\n",
       "              66683,\n",
       "              46961,\n",
       "              73807,\n",
       "              87047,\n",
       "              94407,\n",
       "              27617,\n",
       "              38649,\n",
       "              32750,\n",
       "              56906,\n",
       "              78473,\n",
       "              95931,\n",
       "              103019,\n",
       "              91631,\n",
       "              95311,\n",
       "              84320,\n",
       "              81259,\n",
       "              84636,\n",
       "              84483,\n",
       "              96558,\n",
       "              83269,\n",
       "              92050,\n",
       "              96697,\n",
       "              72317,\n",
       "              27563,\n",
       "              60866,\n",
       "              86394,\n",
       "              90835,\n",
       "              92300,\n",
       "              98024,\n",
       "              115400,\n",
       "              47106,\n",
       "              99860,\n",
       "              110583,\n",
       "              71473,\n",
       "              101487,\n",
       "              51776,\n",
       "              31496,\n",
       "              93796,\n",
       "              95177,\n",
       "              107417,\n",
       "              62832,\n",
       "              114820,\n",
       "              89064,\n",
       "              48407,\n",
       "              74840,\n",
       "              115604,\n",
       "              60545,\n",
       "              64600,\n",
       "              57727,\n",
       "              83676,\n",
       "              98362,\n",
       "              61856,\n",
       "              107145,\n",
       "              39633,\n",
       "              106302,\n",
       "              64831,\n",
       "              50503,\n",
       "              64328,\n",
       "              63151,\n",
       "              69517,\n",
       "              34301,\n",
       "              37564,\n",
       "              46193,\n",
       "              32194,\n",
       "              38061,\n",
       "              48572,\n",
       "              36750,\n",
       "              84807,\n",
       "              62416,\n",
       "              50869,\n",
       "              51295,\n",
       "              60586,\n",
       "              34597,\n",
       "              93425,\n",
       "              40058,\n",
       "              27530,\n",
       "              73413,\n",
       "              84227,\n",
       "              88824,\n",
       "              88836,\n",
       "              90871,\n",
       "              100792,\n",
       "              49177,\n",
       "              51743,\n",
       "              56431,\n",
       "              98343,\n",
       "              101019,\n",
       "              93163,\n",
       "              76570,\n",
       "              117492,\n",
       "              120218,\n",
       "              51713,\n",
       "              63656,\n",
       "              44199,\n",
       "              66096,\n",
       "              71739,\n",
       "              91500,\n",
       "              113987,\n",
       "              93225,\n",
       "              62427,\n",
       "              78885,\n",
       "              109026,\n",
       "              88897,\n",
       "              85201,\n",
       "              62911,\n",
       "              113070,\n",
       "              84844,\n",
       "              81963,\n",
       "              70088,\n",
       "              35271,\n",
       "              95962,\n",
       "              33492,\n",
       "              48041,\n",
       "              80987,\n",
       "              69316,\n",
       "              78187,\n",
       "              124262,\n",
       "              128751,\n",
       "              107446,\n",
       "              29711,\n",
       "              73992,\n",
       "              35227,\n",
       "              74881,\n",
       "              108559,\n",
       "              96318,\n",
       "              78154,\n",
       "              79728,\n",
       "              79810,\n",
       "              86121,\n",
       "              68073,\n",
       "              26811,\n",
       "              58735,\n",
       "              118297,\n",
       "              18569,\n",
       "              95230,\n",
       "              101760,\n",
       "              79175,\n",
       "              105601,\n",
       "              53990,\n",
       "              60884,\n",
       "              34221,\n",
       "              145610,\n",
       "              55539,\n",
       "              81572,\n",
       "              40289,\n",
       "              54061,\n",
       "              74527,\n",
       "              53940,\n",
       "              47001,\n",
       "              74952,\n",
       "              87645,\n",
       "              82609,\n",
       "              44722,\n",
       "              107343,\n",
       "              96836,\n",
       "              40378,\n",
       "              103223,\n",
       "              63340,\n",
       "              98079,\n",
       "              65852,\n",
       "              104545,\n",
       "              42622,\n",
       "              108911,\n",
       "              71171,\n",
       "              30451,\n",
       "              39826,\n",
       "              18941,\n",
       "              57173,\n",
       "              39993,\n",
       "              59137,\n",
       "              63652,\n",
       "              74897,\n",
       "              59674,\n",
       "              38049,\n",
       "              85949,\n",
       "              43063,\n",
       "              32941,\n",
       "              78859,\n",
       "              94360,\n",
       "              129351,\n",
       "              32228,\n",
       "              40977,\n",
       "              58579,\n",
       "              61075,\n",
       "              111213,\n",
       "              59938,\n",
       "              67759,\n",
       "              113979,\n",
       "              87669,\n",
       "              64372,\n",
       "              111820,\n",
       "              93263,\n",
       "              79922,\n",
       "              49186,\n",
       "              60364,\n",
       "              50302]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit_collate = AnonCollate(dataset, cuda=False)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LinearCell(n_in, n_out):\n",
    "    m = nn.Sequential(nn.Linear(n_in, n_out), \n",
    "                      nn.BatchNorm1d(n_out), \n",
    "                      nn.ReLU())\n",
    "    m.out_features = m[0].out_features\n",
    "    return m\n",
    "\n",
    "\n",
    "def Slayer(n_elements, point_dim):\n",
    "    return SLayerRationalHat(n_elements, point_dimension=2, radius_init=0.25)   \n",
    "\n",
    "\n",
    "class AnonModel(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super().__init__()  \n",
    "        \n",
    "        dim_0_n_elements = 50\n",
    "        dim_0_ess_n_elements = 10\n",
    "        dim_1_ess_n_elements = 10\n",
    "        \n",
    "        self.dim_0 = Slayer(dim_0_n_elements, 2)\n",
    "        self.dim_0_ess = Slayer(dim_0_ess_n_elements, 1)\n",
    "        self.dim_1_ess = Slayer(dim_1_ess_n_elements, 1)      \n",
    "        \n",
    "        self.dim_0_linear = LinearCell(dim_0_n_elements, dim_0_n_elements)\n",
    "        self.dim_0_ess_linear = LinearCell(dim_0_ess_n_elements, dim_0_ess_n_elements)\n",
    "        self.dim_1_ess_linear = LinearCell(dim_1_ess_n_elements, dim_1_ess_n_elements)\n",
    "        \n",
    "        cls_in_size = self.dim_0_linear.out_features + self.dim_0_ess_linear.out_features + self.dim_1_ess_linear.out_features\n",
    "        self.classifer = nn.Sequential(\n",
    "                                         nn.BatchNorm1d(cls_in_size),\n",
    "#                                        LinearCell(cls_in_size, cls_in_size),\n",
    "#                                        nn.Dropout(0.2),\n",
    "#                                        LinearCell(cls_in_size, int(cls_in_size/2)),                                        \n",
    "                                         nn.Linear(cls_in_size, train_env.n_target_bins))\n",
    "                         \n",
    "    def forward(self, x):\n",
    "        x_dim_0 = self.dim_0(x['dim_0'])        \n",
    "        x_dim_0 = self.dim_0_linear(x_dim_0)\n",
    "        \n",
    "        x_dim_0_ess = self.dim_0_ess(x['dim_0_ess'])   \n",
    "        x_dim_0_ess = self.dim_0_ess_linear(x_dim_0_ess)\n",
    "        \n",
    "        x_dim_1_ess = self.dim_1_ess(x['dim_1_ess'])       \n",
    "        x_dim_1_ess = self.dim_1_ess_linear(x_dim_1_ess)\n",
    "        \n",
    "        x = torch.cat([x_dim_0, x_dim_0_ess, x_dim_1_ess], dim=1)\n",
    "        \n",
    "        x = self.classifer(x)        \n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def centers_init(self):\n",
    "        dim_0 = []\n",
    "        for i in range(self.dim_0.centers.size(0)):\n",
    "            x = random.uniform(0, 1)\n",
    "            y = random.uniform(0, 1-x)\n",
    "            dim_0.append((x,y))\n",
    "        self.dim_0.centers.data = torch.Tensor(dim_0)\n",
    "        \n",
    "        self.dim_0_ess.centers.data.uniform_(0, 1)\n",
    "        self.dim_1_ess.centers.data.uniform_(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_of_runs = []\n",
    "def experiment():      \n",
    "    splitter = ShuffleSplit(n_splits=10, \n",
    "                            train_size=train_env.train_size, \n",
    "                            test_size=1-train_env.train_size, \n",
    "                            random_state=123)\n",
    "    \n",
    "    train_test_splits = list(splitter.split(X=dataset.targets, y=dataset.targets))\n",
    "    train_test_splits = [(train_i.tolist(), test_i.tolist()) for train_i, test_i in train_test_splits]\n",
    "    \n",
    "    for run_i, (train_i, test_i) in enumerate(train_test_splits):\n",
    "        print('')\n",
    "        print('Run', run_i)\n",
    "        \n",
    "        model = AnonModel()\n",
    "        model.centers_init()\n",
    "        model.cuda()\n",
    "\n",
    "        stats = defaultdict(list)\n",
    "        stats_of_runs.append(stats)\n",
    "        \n",
    "        opt=torch.optim.SGD(model.parameters(), lr=train_env.lr_initial, momentum=0.9)\n",
    "\n",
    "        for i_epoch in range(1, train_env.n_epochs+1):      \n",
    "\n",
    "            model.train()\n",
    "\n",
    "            train_sampler = [i for i in train_i] \n",
    "            random.shuffle(train_sampler)\n",
    "            \n",
    "            dl_train = DataLoader(dataset,\n",
    "                              batch_size=train_env.batch_size, \n",
    "                              collate_fn=reddit_collate,\n",
    "                              sampler=train_sampler, \n",
    "                              num_workers=10)\n",
    "\n",
    "            dl_test = DataLoader(dataset,\n",
    "                                 batch_size=train_env.batch_size, \n",
    "                                 collate_fn=reddit_collate, \n",
    "                                 sampler=test_i, \n",
    "                                 num_workers=10)\n",
    "\n",
    "            epoch_loss = 0\n",
    "\n",
    "            if i_epoch % train_env.lr_epoch_step == 0:\n",
    "                adapt_lr(opt, lambda lr: lr*0.5)\n",
    "\n",
    "            for i_batch, (x, y) in enumerate(dl_train, 1):  \n",
    "                \n",
    "                x = {k: collection_cascade(v,\n",
    "                                       lambda x: isinstance(x, tuple),\n",
    "                                       lambda x: (x[0].cuda(), x[1].cuda(), x[2], x[3]))\n",
    "                     for k, v in x.items()}\n",
    "\n",
    "                y = y.cuda()\n",
    "                \n",
    "                y = torch.autograd.Variable(y)\n",
    "\n",
    "                def closure():\n",
    "                    opt.zero_grad()\n",
    "                    y_hat = model(x)            \n",
    "                    loss = nn.functional.mse_loss(y_hat, y)   \n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "\n",
    "                loss = opt.step(closure)\n",
    "\n",
    "                epoch_loss += float(loss)\n",
    "                stats['loss_by_batch'].append(float(loss))\n",
    "                stats['centers'].append(model.dim_0.centers.data.cpu().numpy())\n",
    "\n",
    "                print(\"Epoch {}/{}, Batch {}/{}\".format(i_epoch, train_env.n_epochs, i_batch, len(dl_train)), end=\"       \\r\")\n",
    "\n",
    "            stats['train_loss_by_epoch'].append(epoch_loss/len(dl_train))\n",
    "\n",
    "            print(\"\\n\\r testing...\")\n",
    "            model.eval()    \n",
    "            true_samples = 0\n",
    "            seen_samples = 0\n",
    "            epoch_test_loss = 0\n",
    "            for i_batch, (x, y) in enumerate(dl_test):\n",
    "                \n",
    "                x = {k: collection_cascade(v,\n",
    "                                       lambda x: isinstance(x, tuple),\n",
    "                                       lambda x: (x[0].cuda(), x[1].cuda(), x[2], x[3]))\n",
    "                     for k, v in x.items()}\n",
    "\n",
    "                y = y.cuda()\n",
    "\n",
    "                y_hat = model(x)\n",
    "                epoch_test_loss += float(nn.functional.mse_loss(y_hat, torch.autograd.Variable(y.cuda())).data) \n",
    "                seen_samples += y.size(0)\n",
    "                \n",
    "            avg_epoch_test_loss = epoch_test_loss/len(dl_test)   \n",
    "            stats['test_loss_by_epoch'].append(avg_epoch_test_loss)            \n",
    "            print(avg_epoch_test_loss)\n",
    "        \n",
    "        stats['tain_i'] = train_i\n",
    "        stats['test_i'] = test_i\n",
    "        stats['model'] = model.cpu()\n",
    "        \n",
    "experiment()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "stats = stats_of_runs[0]\n",
    "c_start = stats['centers'][0]\n",
    "c_end = stats['centers'][-1]\n",
    "\n",
    "plt.plot(c_start[:,0], c_start[:, 1], 'bo', label='center initialization')\n",
    "plt.plot(c_end[:,0], c_end[:, 1], 'ro', label='center learned')\n",
    "\n",
    "all_centers = numpy.stack(stats['centers'], axis=0)\n",
    "for i in range(all_centers.shape[1]):\n",
    "    points = all_centers[:,i, :]\n",
    "    plt.plot(points[:, 0], points[:, 1], '-k')\n",
    "    \n",
    "plt.legend()\n",
    "\n",
    "# for c_i_start, c_i_end in zip(c_start, c_end):\n",
    "#     points = numpy.stack([c_i_start, c_i_end], axis=0)\n",
    "#     plt.plot(points[:,0], points[:,1], '-k')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(stats['train_loss_by_epoch'], label='train_loss')\n",
    "plt.plot(stats['test_loss_by_epoch'], label='test_loss')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
