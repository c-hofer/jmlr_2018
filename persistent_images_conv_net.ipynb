{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import config\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import os.path as pth\n",
    "import shutil\n",
    "import itertools\n",
    "import h5py\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from chofer_tda_datasets import Mpeg7, Animal, Reddit5kJmlr, Reddit12kJmlr, Reininghaus2014ShrecReal, SciNe01EEGBottomTopFiltration\n",
    "from chofer_tda_datasets.transforms import Hdf5GroupToDict, Hdf5GroupToDictSelector\n",
    "from chofer_tda_datasets.utils.h5py_dataset import Hdf5SupervisedDatasetOneFile\n",
    "\n",
    "from jmlr_2018_code.utils import *\n",
    "from persim import PersImage\n",
    "\n",
    "\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        assert isinstance(data, list)\n",
    "        self.data = data\n",
    "        self.targets = [int(y) for y in targets]\n",
    "        \n",
    "        assert len(data) == len(self.targets)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    \n",
    "def compute_persistent_images(dataset, data_keys, spreads=None):\n",
    "    labels = []  \n",
    "    data = []\n",
    "    pixels = [20,20]\n",
    "    for j, (x, y) in enumerate(dataset):\n",
    "        \n",
    "        tens = []\n",
    "        labels.append(y)\n",
    "        for k in data_keys:\n",
    "            \n",
    "            \n",
    "            barcode = x[k]            \n",
    "            tmp = []\n",
    "            \n",
    "            for spread in spreads:\n",
    "                pim = PersImage(pixels=pixels, spread=spread, verbose=False)\n",
    "                \n",
    "                if len(barcode) != 0:\n",
    "                    persistent_image = pim.transform(barcode)\n",
    "                    \n",
    "                else:                    \n",
    "                    persistent_image = np.zeros(pixels)                    \n",
    "                    \n",
    "                tmp.append(persistent_image)\n",
    "            \n",
    "            tens += tmp\n",
    "            \n",
    "        tens = np.stack(tens,axis=0)\n",
    "        data.append(tens.tolist())\n",
    "        \n",
    "        \n",
    "        print(\"Calculating persistence images ... {}/{}\".format(j+1, len(dataset)), end='\\r')\n",
    "    print('')\n",
    "        \n",
    "    return {'data': data, 'targets':labels, 'spreads': spreads}\n",
    "\n",
    "\n",
    "def compute_persistent_images_h5file(file_path, dataset, data_keys, spreads=None):\n",
    "    with h5py.File(file_path, 'w') as h5file:\n",
    "\n",
    "        grp_data = h5file.create_group('data')\n",
    "        \n",
    "        pixels = [20,20]\n",
    "        labels = []\n",
    "        \n",
    "        for j, (x, y) in enumerate(dataset):\n",
    "\n",
    "            tens = []\n",
    "            labels.append(y)\n",
    "            for k in data_keys:\n",
    "\n",
    "                barcode = x[k]            \n",
    "                tmp = []\n",
    "\n",
    "                for spread in spreads:\n",
    "                    pim = PersImage(pixels=pixels, spread=spread, verbose=False)\n",
    "\n",
    "                    if len(barcode) != 0:\n",
    "                        persistent_image = pim.transform(barcode)\n",
    "\n",
    "                    else:                    \n",
    "                        persistent_image = np.zeros(pixels)                    \n",
    "\n",
    "                    tmp.append(persistent_image)\n",
    "\n",
    "                tens += tmp\n",
    "\n",
    "            tens = np.stack(tens,axis=0).astype(np.float32)        \n",
    "        \n",
    "            grp_data.create_dataset(str(j), data=tens)\n",
    "            print(\"Calculating persistence images ... {}/{}\".format(j+1, len(dataset)), end='\\r')\n",
    "\n",
    "\n",
    "        ds_target = h5file.create_dataset('target',\n",
    "                                          dtype=int,\n",
    "                                          data=labels)\n",
    "        \n",
    "           \n",
    "        print('')    \n",
    "\n",
    "\n",
    "def compute_persistent_images_for_datasets():\n",
    "    \n",
    "    root = os.path.join(config.paths.data_root_dir, 'persistent_images')   \n",
    "    spreads = [0.1, 0.5, 1.0]\n",
    "    if not pth.isdir(root):\n",
    "        os.mkdir(root)\n",
    "        \n",
    "    path = pth.join(root, 'mpeg7_pers_img.pickle')\n",
    "    if not pth.isfile(path):\n",
    "        barcode_ds = Mpeg7(config.paths.data_root_dir)\n",
    "        data_keys = [\"dim_0_dir_{}\".format(i) for i in range(0, 32, 2)]\n",
    "        pim = compute_persistent_images(barcode_ds, data_keys, spreads=spreads)    \n",
    "        with open(path, 'bw') as fid:\n",
    "            pickle.dump(pim, fid)        \n",
    "\n",
    "    path = pth.join(root, 'animal_pers_img.pickle')\n",
    "    if not pth.isfile(path):\n",
    "        barcode_ds = Animal(config.paths.data_root_dir)\n",
    "        data_keys = [\"dim_0_dir_{}\".format(i) for i in range(0, 32, 2)]\n",
    "        pim = compute_persistent_images(barcode_ds, data_keys, spreads=spreads)    \n",
    "        with open(path, 'bw') as fid:\n",
    "            pickle.dump(pim, fid)\n",
    "\n",
    "    path = pth.join(root, 'reddit5k_pers_img.pickle')\n",
    "    if not pth.isfile(path):            \n",
    "        barcode_ds = Reddit5kJmlr(config.paths.data_root_dir)\n",
    "        barcode_ds.data_transforms = [Hdf5GroupToDict()]        \n",
    "        barcode_ds.target_transforms = [lambda x: int(x) - 1]\n",
    "        data_keys = ['dim_0']\n",
    "        pim = compute_persistent_images(barcode_ds, data_keys, spreads=spreads)    \n",
    "        with open(path, 'bw') as fid:\n",
    "            pickle.dump(pim, fid)\n",
    "    \n",
    "    path = pth.join(root, 'reddit12k_pers_img.pickle')\n",
    "    if not pth.isfile(path):        \n",
    "        barcode_ds = Reddit12kJmlr(config.paths.data_root_dir)\n",
    "        barcode_ds.data_transforms = [Hdf5GroupToDict()]        \n",
    "        barcode_ds.target_transforms = [lambda x: int(x) - 1]\n",
    "        data_keys = ['dim_0']\n",
    "        pim = compute_persistent_images(barcode_ds, data_keys, spreads=spreads)    \n",
    "        with open(path, 'bw') as fid:\n",
    "            pickle.dump(pim, fid)\n",
    "        \n",
    "    path = pth.join(root, 'shrecReal_pers_img.pickle')\n",
    "    if not pth.isfile(path):  \n",
    "        \n",
    "        def extract_wanted_values(x):\n",
    "            ret = {}\n",
    "            for k, v in x.items():\n",
    "                ret[k] = v['0']\n",
    "\n",
    "            return ret \n",
    "        \n",
    "        barcode_ds = Reininghaus2014ShrecReal(config.paths.data_root_dir)\n",
    "        barcode_ds.data_transforms = [Hdf5GroupToDict(), extract_wanted_values]        \n",
    "        barcode_ds.target_transforms = [lambda x: int(x)]\n",
    "        data_keys = list(barcode_ds[0][0].keys())\n",
    "        pim = compute_persistent_images(barcode_ds, data_keys, spreads=spreads)    \n",
    "        with open(path, 'bw') as fid:\n",
    "            pickle.dump(pim, fid)    \n",
    "            \n",
    "    path = pth.join(root, 'ScineEEG_pers_img.h5')\n",
    "    if not pth.isfile(path):        \n",
    "        barcode_ds = SciNe01EEGBottomTopFiltration(data_root_folder_path=config.paths.data_root_dir)\n",
    "        sensor_indices = [str(i) for i in barcode_ds.sensor_configurations['low_resolution_whole_head']]\n",
    "        selection = {'top': sensor_indices, 'bottom': sensor_indices}\n",
    "        selector = Hdf5GroupToDictSelector(selection)\n",
    "        \n",
    "        def extract_wanted_values(x):\n",
    "            ret = {}\n",
    "            for k, v in x.items():\n",
    "                for kk, vv in v.items():\n",
    "                    ret[k + '_' + kk] = vv\n",
    "\n",
    "            return ret\n",
    "\n",
    "        barcode_ds.data_transforms = [\n",
    "                                       selector,\n",
    "                                       extract_wanted_values\n",
    "                                     ]\n",
    "        barcode_ds.target_transforms = [lambda x: int(x)]\n",
    "        \n",
    "        data_keys = list(barcode_ds[0][0].keys())\n",
    "        compute_persistent_images_h5file(path, barcode_ds, data_keys, spreads=spreads)    \n",
    "\n",
    "        \n",
    "class ScineEEGPersImg(Hdf5SupervisedDatasetOneFile):\n",
    "    file_name = 'ScineEEG_pers_img.h5' \n",
    "    \n",
    "        \n",
    "def pim_ds_factory(ds_name):\n",
    "    \n",
    "    path = pth.join(config.paths.data_root_dir, 'persistent_images') \n",
    "    \n",
    "    if ds_name in ['mpeg7, animal, reddit5k, reddit12k, shrecReal']:\n",
    "    \n",
    "        with open(pth.join(path, ds_name + '_pers_img.pickle'), 'br') as fid:\n",
    "            tmp =  pickle.load(fid)\n",
    "\n",
    "            data = tmp['data']\n",
    "            targets = tmp['targets']\n",
    "\n",
    "            data = [torch.tensor(x) for x in data]\n",
    "\n",
    "            return LabeledDataset(data, targets)\n",
    "        \n",
    "    elif ds_name == 'scine_eeg':           \n",
    "        ds = ScineEEGPersImg(data_root_folder_path=path)\n",
    "        ds.data_transforms = [lambda x: torch.tensor(x)]\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "    else: \n",
    "        raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_persistent_images_for_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShrecRealModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(30,3*30,kernel_size=3,stride=2),\n",
    "            nn.BatchNorm2d(3*30),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(7290,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,40))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mpeg7Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(16*3,64,kernel_size=3,stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(5184,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,70))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "    \n",
    "class AnimalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(16*3,64,kernel_size=3,stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(5184,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,20))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Reddit5kModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1*3,64,kernel_size=3,stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(5184,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,5))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Reddit12kModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1*3,64,kernel_size=3,stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(5184,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,11))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ScineEEGModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(40*3,64,kernel_size=3,stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(5184,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,7))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def model_factory(ds_name):\n",
    "    \n",
    "    if ds_name == 'mpeg7':\n",
    "        return Mpeg7Model()\n",
    "        \n",
    "    elif ds_name == 'animal':\n",
    "        return AnimalModel()\n",
    "        \n",
    "    elif ds_name == 'reddit5k':\n",
    "        return Reddit5kModel()\n",
    "        \n",
    "    elif ds_name == 'reddit12k':\n",
    "        return Reddit12kModel()\n",
    "        \n",
    "    elif ds_name == 'shrecReal':\n",
    "        return ShrecRealModel()\n",
    "    \n",
    "    elif ds_name == 'scine_eeg':\n",
    "        return ScineEEGModel()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(dl_test, net, device='cuda'):\n",
    "\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "    for x,y in dl_test:\n",
    "\n",
    "        x = x.float()\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        outputs = net(x)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return (correct/total)*100\n",
    "\n",
    "\n",
    "def train_and_evaluate(dataset, \n",
    "                model, \n",
    "                epochs= 100, \n",
    "                lr_initial = 0.01, \n",
    "                shedule_stepsize = 50,\n",
    "                shedule_gamme=0.1, \n",
    "                n_processes_collate=None):\n",
    "\n",
    "    device = 'cuda'\n",
    "\n",
    "    train_ratio = 0.9\n",
    "    train_i = np.random.choice(list(range(len(dataset))), \n",
    "                               size=int(len(dataset)*train_ratio), \n",
    "                               replace=False)\n",
    "    test_i = [i for i in range(len(dataset)) if i not in train_i]\n",
    "    assert len(train_i) + len(test_i) == len(dataset)\n",
    "\n",
    "    dl_train = DataLoader(dataset, \n",
    "                          sampler=SubsetRandomSampler(train_i), \n",
    "                          batch_size=100, \n",
    "                          num_workers=0 if n_processes_collate is None else n_processes_collate)\n",
    "\n",
    "    dl_test = DataLoader(dataset, \n",
    "                          sampler=SubsetRandomSampler(test_i), \n",
    "                          batch_size=100,\n",
    "                          num_workers=0 if n_processes_collate is None else n_processes_collate)\n",
    "\n",
    "\n",
    "    net = model.to(device)\n",
    "    n_params = 0\n",
    "    for tmp in net.parameters(): n_params += tmp.numel()\n",
    "    print('#Params: ', n_params)\n",
    "\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=lr_initial)\n",
    "    scheduler = StepLR(optim, step_size=shedule_stepsize, gamma=0.1)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    net.train()\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for x,y in dl_train:\n",
    "            \n",
    "            x = x.float()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            y_hat = net(x)\n",
    "\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print('epoch {}/{}'.format(epoch_i, epochs), end='\\r')\n",
    "        \n",
    "    acc = evaluate_model(dl_test, net, device=device)\n",
    "    print('')\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(ds_name, \n",
    "               n_repititions,\n",
    "               n_processes_collate=None):\n",
    "    dataset = pim_ds_factory(ds_name)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i in range(n_repititions):\n",
    "        model = model_factory(ds_name)\n",
    "        acc = train_and_evaluate(dataset, model, \n",
    "                                 n_processes_collate=n_processes_collate)\n",
    "        print('Run {}: {}'.format(i, acc))\n",
    "        result.append(acc)\n",
    "        \n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params:  3900706\n",
      "epoch 100/100\n",
      "#Params:  3900706\n",
      "epoch 100/100\n",
      "#Params:  3900706\n",
      "epoch 100/100\n",
      "#Params:  3900706\n",
      "epoch 100/100\n",
      "#Params:  3900706\n",
      "epoch 100/100\n",
      "#Params:  3900706\n",
      "epoch 100/100\n",
      "#Params:  3900706\n",
      "epoch 100/100\n",
      "#Params:  3900706\n",
      "epoch 100/100\n",
      "#Params:  3900706\n",
      "epoch 100/100\n",
      "#Params:  3900706\n",
      "epoch 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trained with lr=0.01\n",
    "res = experiment('shrecReal', 10)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params:  2833414\n",
      "epoch 100/100\n",
      "#Params:  2833414\n",
      "epoch 100/100\n",
      "#Params:  2833414\n",
      "epoch 100/100\n",
      "#Params:  2833414\n",
      "epoch 100/100\n",
      "#Params:  2833414\n",
      "epoch 100/100\n",
      "#Params:  2833414\n",
      "epoch 100/100\n",
      "#Params:  2833414\n",
      "epoch 100/100\n",
      "#Params:  2833414\n",
      "epoch 100/100\n",
      "#Params:  2833414\n",
      "epoch 100/100\n",
      "#Params:  2833414\n",
      "epoch 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.35714285714286"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = experiment('mpeg7', 10)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params:  2820564\n",
      "epoch 100/100\n",
      "Run 0: 75.5\n",
      "#Params:  2820564\n",
      "epoch 100/100\n",
      "Run 1: 66.5\n",
      "#Params:  2820564\n",
      "epoch 100/100\n",
      "Run 2: 68.0\n",
      "#Params:  2820564\n",
      "epoch 100/100\n",
      "Run 3: 72.0\n",
      "#Params:  2820564\n",
      "epoch 100/100\n",
      "Run 4: 66.0\n",
      "#Params:  2820564\n",
      "epoch 100/100\n",
      "Run 5: 74.0\n",
      "#Params:  2820564\n",
      "epoch 100/100\n",
      "Run 6: 68.0\n",
      "#Params:  2820564\n",
      "epoch 100/100\n",
      "Run 7: 65.0\n",
      "#Params:  2820564\n",
      "epoch 100/100\n",
      "Run 8: 68.5\n",
      "#Params:  2820564\n",
      "epoch 100/100\n",
      "Run 9: 70.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.35"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = experiment('animal', 10)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params:  2790789\n",
      "epoch 100/100\n",
      "Run 0: 45.0\n",
      "#Params:  2790789\n",
      "epoch 100/100\n",
      "Run 1: 49.0\n",
      "#Params:  2790789\n",
      "epoch 100/100\n",
      "Run 2: 45.0\n",
      "#Params:  2790789\n",
      "epoch 100/100\n",
      "Run 3: 49.4\n",
      "#Params:  2790789\n",
      "epoch 100/100\n",
      "Run 4: 52.400000000000006\n",
      "#Params:  2790789\n",
      "epoch 100/100\n",
      "Run 5: 45.6\n",
      "#Params:  2790789\n",
      "epoch 100/100\n",
      "Run 6: 45.0\n",
      "#Params:  2790789\n",
      "epoch 100/100\n",
      "Run 7: 45.0\n",
      "#Params:  2790789\n",
      "epoch 100/100\n",
      "Run 8: 44.6\n",
      "#Params:  2790789\n",
      "epoch 100/100\n",
      "Run 9: 45.800000000000004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46.68"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = experiment('reddit5k', 10)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params:  2792331\n",
      "epoch 100/100\n",
      "Run 0: 35.5406538139145\n",
      "#Params:  2792331\n",
      "epoch 100/100\n",
      "Run 1: 38.055322715842415\n",
      "#Params:  2792331\n",
      "epoch 100/100\n",
      "Run 2: 30.259849119865883\n",
      "#Params:  2792331\n",
      "epoch 100/100\n",
      "Run 3: 37.63621123218776\n",
      "#Params:  2792331\n",
      "epoch 100/100\n",
      "Run 4: 34.36714165968148\n",
      "#Params:  2792331\n",
      "epoch 100/100\n",
      "Run 5: 32.35540653813915\n",
      "#Params:  2792331\n",
      "epoch 100/100\n",
      "Run 6: 38.558256496228\n",
      "#Params:  2792331\n",
      "epoch 100/100\n",
      "Run 7: 35.20536462699078\n",
      "#Params:  2792331\n",
      "epoch 100/100\n",
      "Run 8: 34.702430846605196\n",
      "#Params:  2792331\n",
      "epoch 100/100\n",
      "Run 9: 34.03185247275775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35.07124895222129"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = experiment('reddit12k', 10)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params:  2858695\n",
      "epoch 100/100\n",
      "Run 0: 30.984126984126988\n",
      "#Params:  2858695\n",
      "epoch 100/100\n",
      "Run 1: 30.22222222222222\n",
      "#Params:  2858695\n",
      "epoch 100/100\n",
      "Run 2: 30.476190476190478\n",
      "#Params:  2858695\n",
      "epoch 100/100\n",
      "Run 3: 29.174603174603174\n",
      "#Params:  2858695\n",
      "epoch 100/100\n",
      "Run 4: 31.492063492063494\n",
      "#Params:  2858695\n",
      "epoch 100/100\n",
      "Run 5: 31.492063492063494\n",
      "#Params:  2858695\n",
      "epoch 100/100\n",
      "Run 6: 29.58730158730159\n",
      "#Params:  2858695\n",
      "epoch 100/100\n",
      "Run 7: 30.698412698412696\n",
      "#Params:  2858695\n",
      "epoch 100/100\n",
      "Run 8: 29.523809523809526\n",
      "#Params:  2858695\n",
      "epoch 100/100\n",
      "Run 9: 30.634920634920636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30.428571428571434"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = experiment('scine_eeg', 10, n_processes_collate=10)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
