{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import config\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import os.path as pth\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from chofer_tda_datasets import Mpeg7, Animal, Reddit5kJmlr, Reddit12kJmlr, Reininghaus2014ShrecReal\n",
    "from chofer_tda_datasets.transforms import Hdf5GroupToDict\n",
    "from persim import PersImage\n",
    "\n",
    "\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        assert isinstance(data, list)\n",
    "        self.data = data\n",
    "        self.targets = [int(y) for y in targets]\n",
    "        \n",
    "        assert len(data) == len(self.targets)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    \n",
    "def compute_persistent_images(dataset, data_keys, spreads=None):\n",
    "    labels = []  \n",
    "    data = []\n",
    "    pixels = [20,20]\n",
    "    for j, (x, y) in enumerate(dataset):\n",
    "        \n",
    "        tens = []\n",
    "        labels.append(y)\n",
    "        for k in data_keys:\n",
    "            \n",
    "            \n",
    "            barcode = x[k]            \n",
    "            tmp = []\n",
    "            \n",
    "            for spread in spreads:\n",
    "                pim = PersImage(pixels=pixels, spread=spread, verbose=False)\n",
    "                \n",
    "                if len(barcode) != 0:\n",
    "                    persistent_image = pim.transform(barcode)\n",
    "                    \n",
    "                else:                    \n",
    "                    persistent_image = np.zeros(pixels)                    \n",
    "                    \n",
    "                tmp.append(persistent_image)\n",
    "            \n",
    "            tens += tmp\n",
    "            \n",
    "        tens = np.stack(tens,axis=0)\n",
    "        data.append(tens.tolist())\n",
    "        \n",
    "        \n",
    "        print(\"Calculating persistence images ... {}/{}\".format(j+1, len(dataset)), end='\\r')\n",
    "    print('')\n",
    "        \n",
    "    return {'data': data, 'targets':labels, 'spreads': spreads}\n",
    "\n",
    "\n",
    "def compute_persistent_images_for_datasets():\n",
    "    \n",
    "    root = os.path.join(config.paths.data_root_dir, 'persistent_images')   \n",
    "    spreads = [0.1, 0.5, 1.0]\n",
    "    if not pth.isdir(root):\n",
    "        os.mkdir(root)\n",
    "        \n",
    "    path = pth.join(root, 'mpeg7_pers_img.pickle')\n",
    "    if not pth.isfile(path):\n",
    "        barcode_ds = Mpeg7(config.paths.data_root_dir)\n",
    "        data_keys = [\"dim_0_dir_{}\".format(i) for i in range(0, 32, 2)]\n",
    "        pim = compute_persistent_images(barcode_ds, data_keys, spreads=spreads)    \n",
    "        with open(path, 'bw') as fid:\n",
    "            pickle.dump(pim, fid)        \n",
    "\n",
    "    path = pth.join(root, 'animal_pers_img.pickle')\n",
    "    if not pth.isfile(path):\n",
    "        barcode_ds = Animal(config.paths.data_root_dir)\n",
    "        data_keys = [\"dim_0_dir_{}\".format(i) for i in range(0, 32, 2)]\n",
    "        pim = compute_persistent_images(barcode_ds, data_keys, spreads=spreads)    \n",
    "        with open(path, 'bw') as fid:\n",
    "            pickle.dump(pim, fid)\n",
    "\n",
    "    path = pth.join(root, 'reddit5k_pers_img.pickle')\n",
    "    if not pth.isfile(path):            \n",
    "        barcode_ds = Reddit5kJmlr(config.paths.data_root_dir)\n",
    "        barcode_ds.data_transforms = [Hdf5GroupToDict()]        \n",
    "        barcode_ds.target_transforms = [lambda x: int(x) - 1]\n",
    "        data_keys = ['dim_0']\n",
    "        pim = compute_persistent_images(barcode_ds, data_keys, spreads=spreads)    \n",
    "        with open(path, 'bw') as fid:\n",
    "            pickle.dump(pim, fid)\n",
    "    \n",
    "    path = pth.join(root, 'reddit12k_pers_img.pickle')\n",
    "    if not pth.isfile(path):        \n",
    "        barcode_ds = Reddit12kJmlr(config.paths.data_root_dir)\n",
    "        barcode_ds.data_transforms = [Hdf5GroupToDict()]        \n",
    "        barcode_ds.target_transforms = [lambda x: int(x) - 1]\n",
    "        data_keys = ['dim_0']\n",
    "        pim = compute_persistent_images(barcode_ds, data_keys, spreads=spreads)    \n",
    "        with open(path, 'bw') as fid:\n",
    "            pickle.dump(pim, fid)\n",
    "        \n",
    "#     path = pth.join(root, 'shrecReal_pers_img.pickle')\n",
    "\n",
    "\n",
    "def pim_ds_factory(ds_name):\n",
    "    \n",
    "    path = pth.join(config.paths.data_root_dir, 'persistent_images')   \n",
    "    \n",
    "    with open(pth.join(path, ds_name + '_pers_img.pickle'), 'br') as fid:\n",
    "        tmp =  pickle.load(fid)\n",
    "        \n",
    "        data = tmp['data']\n",
    "        targets = tmp['targets']\n",
    "        \n",
    "        data = [torch.tensor(x) for x in data]\n",
    "        \n",
    "        return LabeledDataset(data, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating persistence images ... 8424/11929\r"
     ]
    }
   ],
   "source": [
    "compute_persistent_images_for_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mpeg7Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(16*3,64,kernel_size=3,stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(5184,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,70))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "    \n",
    "class AnimalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(16*3,64,kernel_size=3,stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(5184,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,20))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Reddit5kModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1*3,64,kernel_size=3,stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(5184,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,5))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Reddit12kModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1*3,64,kernel_size=3,stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(5184,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,11))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.clf(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def model_factory(ds_name):\n",
    "    \n",
    "    if ds_name == 'mpeg7':\n",
    "        return Mpeg7Model()\n",
    "        \n",
    "    elif ds_name == 'animal':\n",
    "        return AnimalModel()\n",
    "        \n",
    "    elif ds_name == 'reddit5k':\n",
    "        return Reddit5kModel()\n",
    "        \n",
    "    elif ds_name == 'reddit12k':\n",
    "        return Reddit12kModel()\n",
    "        \n",
    "    elif ds_name == 'shrecReal':\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(dl_test, net, device='cuda'):\n",
    "\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "    for x,y in dl_test:\n",
    "\n",
    "        x = x.float()\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        outputs = net(x)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return (correct/total)*100\n",
    "\n",
    "\n",
    "def train_and_evaluate(dataset, \n",
    "                model, \n",
    "                epochs= 100, \n",
    "                lr_initial = 0.0001, \n",
    "                shedule_stepsize = 50,\n",
    "                shedule_gamme=0.1):\n",
    "\n",
    "    device = 'cuda'\n",
    "\n",
    "    train_ratio = 0.9\n",
    "    train_i = np.random.choice(list(range(len(dataset))), \n",
    "                               size=int(len(dataset)*train_ratio), \n",
    "                               replace=False)\n",
    "    test_i = [i for i in range(len(dataset)) if i not in train_i]\n",
    "    assert len(train_i) + len(test_i) == len(dataset)\n",
    "\n",
    "    dl_train = DataLoader(dataset, \n",
    "                          sampler=SubsetRandomSampler(train_i), \n",
    "                          batch_size=100)\n",
    "\n",
    "    dl_test = DataLoader(dataset, \n",
    "                          sampler=SubsetRandomSampler(test_i), \n",
    "                          batch_size=100)\n",
    "\n",
    "\n",
    "    net = model.to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=lr_initial)\n",
    "    scheduler = StepLR(optim, step_size=shedule_stepsize, gamma=0.1)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    net.train()\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for x,y in dl_train:\n",
    "            \n",
    "            x = x.float()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            y_hat = net(x)\n",
    "\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print('epoch {}/{}'.format(epoch_i, epochs), end='\\r')\n",
    "        \n",
    "    acc = evaluate_model(dl_test, net, device=device)\n",
    "    print('')\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(ds_name, n_repititions):\n",
    "    dataset = pim_ds_factory(ds_name)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i in range(n_repititions):\n",
    "        model = model_factory(ds_name)\n",
    "        acc = train_and_evaluate(dataset, model)\n",
    "        \n",
    "        result.append(acc)\n",
    "        \n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.57142857142858"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = experiment('mpeg7', 10)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65.85"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = experiment('animal', 10)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48.42"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = experiment('reddit5k', 10)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n",
      "epoch 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38.90192791282482"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = experiment('reddit12k', 10)\n",
    "np.mean(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
